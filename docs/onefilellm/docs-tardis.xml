<onefilellm_output>
<source type="web_crawl" base_url="https://docs.tardis.dev/">
<page url="https://docs.tardis.dev/">
<title>Welcome | Tardis.dev Documentation</title>
<content><heading level="1">Welcome</heading><paragraph>Welcome to Tardis.dev documentation pages</paragraph></content>
</page>
<page url="https://docs.tardis.dev/faq">
<title>Frequently Asked Questions | Tardis.dev Documentation</title>
<content><heading level="1">Frequently Asked Questions</heading><paragraph>Got questions? We're happy to help!</paragraph></content>
</page>
<page url="https://docs.tardis.dev/downloadable-csv-files">
<title>Downloadable CSV files | Tardis.dev Documentation</title>
<content><paragraph>CSV datasets are available via dedicated datasets API that allows downloading tick level incremental order book L2 updates, order book snapshots, trades, options chains, quotes, derivative tickers and liquidations data.</paragraph><paragraph>For ongoing data, CSV datasets for a given day are available on the next day around 06:00 UTC.</paragraph><paragraph>Historical datasets for the first day of each month are available to download without API key.</paragraph><paragraph>Our Node.js and Python clients have built-in functions to efficiently download whole date range of data.</paragraph><code language="python"># pip install tardis-dev
# requires Python >=3.6
from tardis_dev import datasets

datasets.download(
    exchange="deribit",
    data_types=[
        "incremental_book_L2",
        "trades",
        "quotes",
        "derivative_ticker",
        "book_snapshot_25",
        "liquidations"
    ],
    from_date="2019-11-01",
    to_date="2019-11-02",
    symbols=["BTC-PERPETUAL", "ETH-PERPETUAL"],
    api_key="YOUR API KEY (optionally)",
)</code><paragraph>See full example **that shows all available download options (**download path customization, filenames conventions and more).</paragraph><paragraph>See full example **that shows all available download options (**download path customization, filenames conventions and more).</paragraph><paragraph>CSV format details</paragraph><list type="ul"><item>columns delimiter: , (comma)</item></list><paragraph>columns delimiter: , (comma)</paragraph><paragraph>• incremental_book_L2</paragraph><paragraph>Incremental order book L2 updates collected from exchanges' real-time WebSocket order book L2 data feeds - data as deep and granular as underlying real-time data source, please see FAQ: What is the maximum order book depth available for each supported exchange? for more details.</paragraph><paragraph>As exchanges real-time feeds usually publish multiple order book levels updates via single message you can recognize that by grouping rows by local_timestamp field if needed.</paragraph><paragraph>CSV incremental_book_L2 schema</paragraph><paragraph>dataset preview</paragraph><paragraph>instrument symbol as provided by exchange (always uppercase)</paragraph><paragraph>timestamp provided by exchange in microseconds since epoch - if exchange does not provide one local_timestamp value is used as a fallback</paragraph><paragraph>message arrival timestamp in microseconds since epoch</paragraph><paragraph>possible values:</paragraph><list type="ul"><item>true - if update was a part of initial order book snapshot</item><item>false - if update was not a part of initial order book snapshot</item></list><paragraph>true - if update was a part of initial order book snapshot</paragraph><paragraph>false - if update was not a part of initial order book snapshot</paragraph><paragraph>If last update was not a snapshot and current one is, then existing order book state must be discarded (all existing levels removed)</paragraph><paragraph>determines to which side of the order book update belongs to:</paragraph><list type="ul"><item>bid - bid side of the book, buy orders</item><item>ask - ask side of the book, sell orders</item></list><paragraph>bid - bid side of the book, buy orders</paragraph><paragraph>ask - ask side of the book, sell orders</paragraph><paragraph>price identifying book level being updated</paragraph><paragraph>updated price level amount as provided by exchange, not a delta - an amount of 0 indicates that the price level can be removed</paragraph><paragraph>exchange</paragraph><paragraph>symbol</paragraph><paragraph>timestamp</paragraph><paragraph>local_timestamp</paragraph><paragraph>is_snapshot</paragraph><paragraph>side</paragraph><paragraph>price</paragraph><paragraph>amount</paragraph><paragraph>• book_snapshot_25</paragraph><paragraph>Tick-level order book snapshots reconstructed from exchanges' real-time WebSocket order book L2 data feeds. Each row represents top 25 levels from each side of the limit order book book and was recorded every time any of the tracked bids/asks top 25 levels have changed.</paragraph><paragraph>CSV book_snapshot_25 schema</paragraph><paragraph>dataset preview</paragraph><paragraph>instrument symbol as provided by exchange (always uppercase)</paragraph><paragraph>timestamp provided by exchange in microseconds since epoch - if exchange does not provide one local_timestamp value is used as a fallback</paragraph><paragraph>message arrival timestamp in microseconds since epoch</paragraph><paragraph>top 25 asks prices in ascending order, empty if there aren't enough price levels available in the order book or provided by the exchange</paragraph><paragraph>top 25 asks amounts in ascending order, empty if there aren't enough price levels available in the order book or provided by the exchange</paragraph><paragraph>top 25 bids prices in descending order, empty if there aren't enough price levels available in the order book or provided by the exchange</paragraph><paragraph>top 25 bids amounts in descending order, empty if there aren't enough price levels available in the order book or provided by the exchange</paragraph><paragraph>exchange</paragraph><paragraph>symbol</paragraph><paragraph>timestamp</paragraph><paragraph>local_timestamp</paragraph><paragraph>asks[0].price</paragraph><paragraph>asks[0].amount</paragraph><paragraph>bids[0].price</paragraph><paragraph>bids[0].amount</paragraph><paragraph>asks[1].price</paragraph><paragraph>asks[1].amount</paragraph><paragraph>bids[1].price</paragraph><paragraph>bids[1].amount</paragraph><paragraph>asks[2].price</paragraph><paragraph>asks[2].amount</paragraph><paragraph>bids[2].price</paragraph><paragraph>bids[2].amount</paragraph><paragraph>asks[3].price</paragraph><paragraph>asks[3].amount</paragraph><paragraph>bids[3].price</paragraph><paragraph>bids[3].amount</paragraph><paragraph>asks[4].price</paragraph><paragraph>asks[4].amount</paragraph><paragraph>bids[4].price</paragraph><paragraph>bids[4].amount</paragraph><paragraph>asks[5].price</paragraph><paragraph>asks[5].amount</paragraph><paragraph>bids[5].price</paragraph><paragraph>bids[5].amount</paragraph><paragraph>asks[6].price</paragraph><paragraph>asks[6].amount</paragraph><paragraph>bids[6].price</paragraph><paragraph>bids[6].amount</paragraph><paragraph>asks[7].price</paragraph><paragraph>asks[7].amount</paragraph><paragraph>bids[7].price</paragraph><paragraph>bids[7].amount</paragraph><paragraph>asks[8].price</paragraph><paragraph>asks[8].amount</paragraph><paragraph>bids[8].price</paragraph><paragraph>bids[8].amount</paragraph><paragraph>asks[9].price</paragraph><paragraph>asks[9].amount</paragraph><paragraph>bids[9].price</paragraph><paragraph>bids[9].amount</paragraph><paragraph>asks[10].price</paragraph><paragraph>asks[10].amount</paragraph><paragraph>bids[10].price</paragraph><paragraph>bids[10].amount</paragraph><paragraph>asks[11].price</paragraph><paragraph>asks[11].amount</paragraph><paragraph>bids[11].price</paragraph><paragraph>bids[11].amount</paragraph><paragraph>asks[12].price</paragraph><paragraph>asks[12].amount</paragraph><paragraph>bids[12].price</paragraph><paragraph>bids[12].amount</paragraph><paragraph>asks[13].price</paragraph><paragraph>asks[13].amount</paragraph><paragraph>bids[13].price</paragraph><paragraph>bids[13].amount</paragraph><paragraph>asks[14].price</paragraph><paragraph>asks[14].amount</paragraph><paragraph>bids[14].price</paragraph><paragraph>bids[14].amount</paragraph><paragraph>asks[15].price</paragraph><paragraph>asks[15].amount</paragraph><paragraph>bids[15].price</paragraph><paragraph>bids[15].amount</paragraph><paragraph>asks[16].price</paragraph><paragraph>asks[16].amount</paragraph><paragraph>bids[16].price</paragraph><paragraph>bids[16].amount</paragraph><paragraph>asks[17].price</paragraph><paragraph>asks[17].amount</paragraph><paragraph>bids[17].price</paragraph><paragraph>bids[17].amount</paragraph><paragraph>asks[18].price</paragraph><paragraph>asks[18].amount</paragraph><paragraph>bids[18].price</paragraph><paragraph>bids[18].amount</paragraph><paragraph>asks[19].price</paragraph><paragraph>asks[19].amount</paragraph><paragraph>bids[19].price</paragraph><paragraph>bids[19].amount</paragraph><paragraph>asks[20].price</paragraph><paragraph>asks[20].amount</paragraph><paragraph>bids[20].price</paragraph><paragraph>bids[20].amount</paragraph><paragraph>asks[21].price</paragraph><paragraph>asks[21].amount</paragraph><paragraph>bids[21].price</paragraph><paragraph>bids[21].amount</paragraph><paragraph>asks[22].price</paragraph><paragraph>asks[22].amount</paragraph><paragraph>bids[22].price</paragraph><paragraph>bids[22].amount</paragraph><paragraph>asks[23].price</paragraph><paragraph>asks[23].amount</paragraph><paragraph>bids[23].price</paragraph><paragraph>bids[23].amount</paragraph><paragraph>asks[24].price</paragraph><paragraph>asks[24].amount</paragraph><paragraph>bids[24].price</paragraph><paragraph>bids[24].amount</paragraph><paragraph>• book_snapshot_5</paragraph><paragraph>Tick-level order book snapshots reconstructed from exchanges' real-time WebSocket order book L2 data feeds. Each row represents top 5 levels from each side of the limit order book book and was recorded every time any of the tracked bids/asks top 5 levels have changed.</paragraph><paragraph>CSV book_snapshot_5 schema</paragraph><paragraph>dataset preview</paragraph><paragraph>instrument symbol as provided by exchange (always uppercase)</paragraph><paragraph>timestamp provided by exchange in microseconds since epoch - if exchange does not provide one local_timestamp value is used as a fallback</paragraph><paragraph>message arrival timestamp in microseconds since epoch</paragraph><paragraph>top 5 asks prices in ascending order, empty if there aren't enough price levels available in the order book or provided by the exchange</paragraph><paragraph>top 5 asks amounts in ascending order, empty if there aren't enough price levels available in the order book or provided by the exchange</paragraph><paragraph>top 5 bids prices in descending order, empty if there aren't enough price levels available in the order book or provided by the exchange</paragraph><paragraph>top 5 bids amounts in descending order, empty if there aren't enough price levels available in the order book or provided by the exchange</paragraph><paragraph>exchange</paragraph><paragraph>symbol</paragraph><paragraph>timestamp</paragraph><paragraph>local_timestamp</paragraph><paragraph>asks[0].price</paragraph><paragraph>asks[0].amount</paragraph><paragraph>bids[0].price</paragraph><paragraph>bids[0].amount</paragraph><paragraph>asks[1].price</paragraph><paragraph>asks[1].amount</paragraph><paragraph>bids[1].price</paragraph><paragraph>bids[1].amount</paragraph><paragraph>asks[2].price</paragraph><paragraph>asks[2].amount</paragraph><paragraph>bids[2].price</paragraph><paragraph>bids[2].amount</paragraph><paragraph>asks[3].price</paragraph><paragraph>asks[3].amount</paragraph><paragraph>bids[3].price</paragraph><paragraph>bids[3].amount</paragraph><paragraph>asks[4].price</paragraph><paragraph>asks[4].amount</paragraph><paragraph>Individual trades data collected from exchanges' real-time WebSocket trades data feeds.</paragraph><paragraph>CSV trades schema</paragraph><paragraph>dataset preview</paragraph><paragraph>instrument symbol as provided by exchange (always uppercase)</paragraph><paragraph>timestamp provided by exchange in microseconds since epoch - if exchange does not provide one local_timestamp value is used as a fallback</paragraph><paragraph>message arrival timestamp in microseconds since epoch</paragraph><paragraph>trade id as provided by exchange, empty if exchange does not provide one - different exchanges provide id's as numeric values, GUID's or other strings, and some do not provide that information at all</paragraph><paragraph>liquidity taker side (aggressor), possible values:</paragraph><list type="ul"><item>buy - liquidity taker was buying</item><item>sell - liquidity taker was selling</item><item>unknown - exchange did not provide that information</item></list><paragraph>buy - liquidity taker was buying</paragraph><paragraph>sell - liquidity taker was selling</paragraph><paragraph>unknown - exchange did not provide that information</paragraph><paragraph>trade price as provided by exchange</paragraph><paragraph>trade amount as provided by exchange</paragraph><paragraph>exchange</paragraph><paragraph>symbol</paragraph><paragraph>timestamp</paragraph><paragraph>local_timestamp</paragraph><paragraph>id</paragraph><paragraph>side</paragraph><paragraph>price</paragraph><paragraph>amount</paragraph><paragraph>• options_chain</paragraph><paragraph>Tick-level options summary info (strike prices, expiration dates, open interest, implied volatility, greeks etc.) for all active options instruments collected from exchanges' real-time WebSocket options tickers data feeds. Options chain data is available for Deribit (sourced from ticker channel) and OKEx Options (sourced from option/summary and index/ticker channels).</paragraph><paragraph>CSV options_chain schema</paragraph><paragraph>dataset preview</paragraph><paragraph>instrument symbol as provided by exchange (always uppercase)</paragraph><paragraph>ticker timestamp provided by exchange in microseconds since epoch</paragraph><paragraph>ticker message arrival timestamp in microseconds since epoch</paragraph><paragraph>option type, possible values:</paragraph><paragraph>option expiration date in microseconds since epoch</paragraph><paragraph>current open interest, empty is exchange does not provide one</paragraph><paragraph>price of the last trade, empty if there weren't any trades yet</paragraph><paragraph>current best bid price, empty if there aren't any bids</paragraph><paragraph>current best bid amount, empty if there aren't any bids</paragraph><paragraph>implied volatility for best bid, empty if there aren't any bids</paragraph><paragraph>current best ask price, empty if there aren't any asks</paragraph><paragraph>current best ask amount, empty if there aren't any asks</paragraph><paragraph>implied volatility for best ask, empty if there aren't any asks</paragraph><paragraph>mark price, empty is exchange does not provide one</paragraph><paragraph>implied volatility for mark price, empty is exchange does not provide one</paragraph><paragraph>underlying index name that option contract is based upon</paragraph><paragraph>underlying price, empty is exchange does not provide one</paragraph><paragraph>delta value for the option, empty is exchange does not provide one</paragraph><paragraph>gamma value for the option, empty is exchange does not provide one</paragraph><paragraph>vega value for the option, empty is exchange does not provide one</paragraph><paragraph>theta value for the option, empty is exchange does not provide one</paragraph><paragraph>rho value for the option, empty is exchange does not provide one</paragraph><paragraph>exchange</paragraph><paragraph>symbol</paragraph><paragraph>timestamp</paragraph><paragraph>local_timestamp</paragraph><paragraph>type</paragraph><paragraph>strike_price</paragraph><paragraph>expiration</paragraph><paragraph>open_interest</paragraph><paragraph>last_price</paragraph><paragraph>bid_price</paragraph><paragraph>bid_amount</paragraph><paragraph>bid_iv</paragraph><paragraph>ask_price</paragraph><paragraph>ask_amount</paragraph><paragraph>ask_iv</paragraph><paragraph>mark_price</paragraph><paragraph>mark_iv</paragraph><paragraph>underlying_index</paragraph><paragraph>underlying_price</paragraph><paragraph>delta</paragraph><paragraph>gamma</paragraph><paragraph>vega</paragraph><paragraph>theta</paragraph><paragraph>rho</paragraph><paragraph>Top of the book (best bid/ask) data reconstructed from exchanges' real-time WebSocket order book L2 data feeds. - best bid/ask recorded every time top of the book has changed.</paragraph><paragraph>We on purpose choose this solution over native exchanges real-time quotes feeds as those vary a lot between exchanges, can be throttled, some are absent at all, often are delayed and published in batches in comparison to more granular L2 updates which are the basis for our quotes dataset.</paragraph><paragraph>CSV quotes schema</paragraph><paragraph>dataset preview</paragraph><paragraph>instrument symbol as provided by exchange (always uppercase)</paragraph><paragraph>timestamp provided by exchange in microseconds since epoch - if exchange does not provide one local_timestamp value is used as a fallback</paragraph><paragraph>message arrival timestamp in microseconds since epoch</paragraph><paragraph>best ask amount as provided by exchange, empty if there aren't any asks</paragraph><paragraph>best ask price as provided by exchange, empty if there aren't any asks</paragraph><paragraph>best bid price as provided by exchange, empty if there aren't any bids</paragraph><paragraph>best bid amount as provided by exchange, empty if there aren't any bids</paragraph><paragraph>exchange</paragraph><paragraph>symbol</paragraph><paragraph>timestamp</paragraph><paragraph>local_timestamp</paragraph><paragraph>ask_amount</paragraph><paragraph>ask_price</paragraph><paragraph>bid_price</paragraph><paragraph>bid_amount</paragraph><paragraph>book_ticker</paragraph><paragraph>• derivative_ticker</paragraph><paragraph>Derivative instrument ticker info (open interest, funding, mark price, index price) collected from exchanges' real-time WebSocket instruments & tickers data feeds.</paragraph><paragraph>Anytime any of the tracked values has changed data was added to final dataset.</paragraph><paragraph>CSV derivative_ticker schema</paragraph><paragraph>dataset preview</paragraph><paragraph>instrument symbol as provided by exchange (always uppercase)</paragraph><paragraph>timestamp provided by exchange in microseconds since epoch - if exchange does not provide one local_timestamp value is used as a fallback</paragraph><paragraph>message arrival timestamp in microseconds since epoch</paragraph><paragraph>timestamp of the next funding event in microseconds since epoch, empty if exchange does not provide one</paragraph><paragraph>funding rate that will take effect on the next funding event at funding timestamp, for some exchanges it's fixed, for other it fluctuates, empty if exchange does not provide one</paragraph><paragraph>estimated predicted funding rate for the next after closest funding event, empty if exchange does not provide one</paragraph><paragraph>current open interest, empty if exchange does not provide one</paragraph><paragraph>last instrument price, empty if exchange does not provide one</paragraph><paragraph>index price of the instrument, empty if exchange does not provide one</paragraph><paragraph>mark price of the instrument, empty if exchange does not provide one</paragraph><paragraph>1</paragraph><paragraph>exchange</paragraph><paragraph>symbol</paragraph><paragraph>timestamp</paragraph><paragraph>local_timestamp</paragraph><paragraph>funding_timestamp</paragraph><paragraph>funding_rate</paragraph><paragraph>predicted_funding_rate</paragraph><paragraph>open_interest</paragraph><paragraph>last_price</paragraph><paragraph>index_price</paragraph><paragraph>mark_price</paragraph><paragraph>Liquidations data collected from exchanges' real-time WebSocket data feeds were available.</paragraph><paragraph>See details which exchanges support it and since when.</paragraph><paragraph>CSV liquidations schema</paragraph><paragraph>dataset preview</paragraph><paragraph>instrument symbol as provided by exchange (always uppercase)</paragraph><paragraph>timestamp provided by exchange in microseconds since epoch - if exchange does not provide one local_timestamp value is used as a fallback</paragraph><paragraph>message arrival timestamp in microseconds since epoch</paragraph><paragraph>liquidation id as provided by exchange, empty if exchange does not provide one - different exchanges provide id's as numeric values, GUID's or other strings, and some do not provide that information at all</paragraph><paragraph>liquidation side:</paragraph><list type="ul"><item>buy - short position was liquidated</item><item>sell - long position was liquidated</item></list><paragraph>buy - short position was liquidated</paragraph><paragraph>sell - long position was liquidated</paragraph><paragraph>liquidation price as provided by exchange</paragraph><paragraph>liquidation amount as provided by exchange</paragraph><paragraph>exchange</paragraph><paragraph>symbol</paragraph><paragraph>timestamp</paragraph><paragraph>local_timestamp</paragraph><paragraph>id</paragraph><paragraph>side</paragraph><paragraph>price</paragraph><paragraph>amount</paragraph><paragraph>Grouped symbols</paragraph><paragraph>In addition to standard currency pairs & instrument symbols that can be requested when via CSV datasets API, each exchange has additional special grouped symbols available depending if it supports given market type: SPOT, FUTURES, OPTIONS and PERPETUALS. When such symbol is requested then downloaded file for it has all the data for all instruments belonging for given market type. This is especially useful for options instruments that as specifying each option symbol one by one can be mundane process, using 'OPTIONS' as a symbol gives data for all options available at given time.</paragraph><list type="ul"><item>trades - available for SPOT, FUTURES, OPTIONS, PERPETUALS</item><item>quotes - available for OPTIONS</item></list><paragraph>trades - available for SPOT, FUTURES, OPTIONS, PERPETUALS</paragraph><paragraph>quotes - available for OPTIONS</paragraph><paragraph>those special symbols are also listed in response to /exchanges/:exchange API call</paragraph><paragraph>Datasets API details</paragraph><list type="ul"><item>all downloadable datasets are gzip compressed</item><item>historical market data is available in daily intervals (separate file for each day) based on local timestamp (timestamp of message arrival) split by exchange, data type and symbol</item><item>datasets are ordered and split into separate daily files by local_timestamp (timestamp of message arrival time)</item><item>empty gzip compressed file is being returned in case of no data available for a given day, symbol and data type, e.g., exchange downtime, very low volume currency pairs etc.</item><item>iftimestamp equals to local_timestamp it means that exchange didn't provide timestamp for message, e.g., BitMEX order book updates</item><item>cell in CSV file is empty if there's no value for it, e.g., no trade id if a given exchange doesn't provide one</item></list><paragraph>all downloadable datasets are gzip compressed</paragraph><paragraph>historical market data is available in daily intervals (separate file for each day) based on local timestamp (timestamp of message arrival) split by exchange, data type and symbol</paragraph><paragraph>datasets are ordered and split into separate daily files by local_timestamp (timestamp of message arrival time)</paragraph><paragraph>empty gzip compressed file is being returned in case of no data available for a given day, symbol and data type, e.g., exchange downtime, very low volume currency pairs etc.</paragraph><paragraph>iftimestamp equals to local_timestamp it means that exchange didn't provide timestamp for message, e.g., BitMEX order book updates</paragraph><paragraph>cell in CSV file is empty if there's no value for it, e.g., no trade id if a given exchange doesn't provide one</paragraph><paragraph>Download via client libraries</paragraph><paragraph>Datasets API reference</paragraph><paragraph>GET https://datasets.tardis.dev/v1/:exchange/:dataType/:year/:month/:day/:symbol.csv.gz</paragraph><paragraph>Returns gzip compressed CSV dataset for given exchange, data type, date (year, month, day) and symbol.</paragraph><paragraph>Path Parameters</paragraph><paragraph>one of https://api.tardis.dev/v1/exchanges (field id, only exchanges with "supportsDatasets":true)</paragraph><paragraph>one of datasets.symbols[].dataTypes values from https://api.tardis.dev/v1/exchanges/:exchange API response</paragraph><paragraph>year in format YYYY (four-digit year)</paragraph><paragraph>month in format MM (two-digit month of the year)</paragraph><paragraph>day in format DD (two-digit day of the month)</paragraph><paragraph>one of datasets.symbols[].id values from https://api.tardis.dev/v1/exchanges/:exchange API response, see details below</paragraph><paragraph>For authenticated requests provide Authorization header with value: 'Bearer YOUR_API_KEY'. Without API key historical datasets for the first day of each month are available to download.</paragraph><paragraph>200 gzip compressed CSV dataset</paragraph><list type="ul"><item>symbols param provided to datasets API in comparison to HTTP API needs to be both always uppercase and have '/' and ':' characters replaced with '-' so symbol is url safe.</item></list><paragraph>symbols param provided to datasets API in comparison to HTTP API needs to be both always uppercase and have '/' and ':' characters replaced with '-' so symbol is url safe.</paragraph><paragraph>Sample requests</paragraph></content>
</page>
<page url="https://docs.tardis.dev/api/getting-started">
<title>Getting Started | Tardis.dev Documentation</title>
<content><list type="ol"><item>API</item></list><heading level="1">Getting Started</heading><paragraph>Overview of the main ways Tardis.dev historical market data can be accessed programmatically</paragraph></content>
</page>
<page url="https://docs.tardis.dev/api/python">
<title>Python Client | Tardis.dev Documentation</title>
<content><code language="python">import asyncio
import csv
from tardis_client import TardisClient, Channel


async def save_historical_deribit_index_data_to_csv():
    tardis_client = TardisClient()

    messages = tardis_client.replay(
        exchange="deribit",
        from_date="2019-06-01",
        to_date="2019-06-02",
        filters=[Channel(name="deribit_price_index", symbols=["btc_usd", "eth_usd"])],
    )
    with open("./deribit_index_data.csv", mode="w") as csv_file:
        fieldnames = ["symbol", "price", "timestamp"]
        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
        writer.writeheader()
        async for local_timestamp, message in messages:
            data = message["params"]["data"]
            writer.writerow({"symbol": data["index_name"], "price": data["price"], "timestamp": data["timestamp"]})

    print("finished")


asyncio.run(save_historical_deribit_index_data_to_csv())
</code></content>
</page>
<page url="https://docs.tardis.dev/api/node-js">
<title>Node.js Client | Tardis.dev Documentation</title>
<content><paragraph>Node.js tardis-dev library provides convenient access to tick-level historical and real-time cryptocurrency market data both in exchange-native and normalized formats. Instead of callbacks it relies on async iteration (for await ...of)arrow-up-right enabling composability features like seamless switching between real-time data streaming and historical data replay or computing derived data locally.</paragraph><paragraph>historical market data replay</paragraph><paragraph>real-time market data streaming</paragraph><code language="javascript">const tardis = require('tardis-dev')
const { replayNormalized, normalizeTrades, normalizeBookChanges } = tardis

const messages = replayNormalized(
  {
    exchange: 'bitmex',
    symbols: ['XBTUSD', 'ETHUSD'],
    from: '2019-05-01',
    to: '2019-05-02'
  },
  normalizeTrades,
  normalizeBookChanges
)

for await (const message of messages) {
  console.log(message)
}</code><code language="javascript">const tardis = require('tardis-dev')
const { streamNormalized, normalizeTrades, normalizeBookChanges } = tardis

const messages = streamNormalized(
  {
    exchange: 'bitmex',
    symbols: ['XBTUSD', 'ETHUSD']
  },
  normalizeTrades,
  normalizeBookChanges
)

for await (const message of messages) {
  console.log(message)
}</code><paragraph>arrow-up-right</paragraph><list type="ul"><item>historical tick-level market data replay backed by Tardis.dev HTTP API — includes full order book depth snapshots plus incremental updates, trades, historical open interest, funding, index, mark prices, liquidations and more</item><item>support for both exchange-native and normalized market data formats (unified format for accessing market data across all supported exchanges — normalized trades, order book and ticker data)</item><item>transparent historical local data caching (cached data is stored on disk in compressed GZIP format and decompressed on demand when reading the data)</item><item>support for top cryptocurrency exchanges: BitMEX, Deribit, Binance, Binance Futures, FTX, OKEx, Huobi Global, Huobi DM, bitFlyer, Bitstamp, Coinbase Pro, Kraken Futures, Gemini, Kraken, Bitfinex, Bybit, OKCoin, CoinFLEX and more</item><item>automatic closed connections and stale connections reconnection logic for real-time streams</item><item>computing derived data locally like order book imbalance, customizable trade bars, book snapshots and more via compute helper function and computables, e.g., volume based bars, top 20 levels order book snapshots taken every 10 ms etc</item><item>fast and lightweight architecture — low memory footprint and no heavy in-memory buffering</item><item>built-in TypeScript support</item></list><paragraph>historical tick-level market data replay backed by Tardis.dev HTTP API — includes full order book depth snapshots plus incremental updates, trades, historical open interest, funding, index, mark prices, liquidations and more</paragraph><paragraph>support for both exchange-native and normalized market data formats (unified format for accessing market data across all supported exchanges — normalized trades, order book and ticker data)</paragraph><paragraph>transparent historical local data caching (cached data is stored on disk in compressed GZIP format and decompressed on demand when reading the data)</paragraph><paragraph>support for top cryptocurrency exchanges: BitMEX, Deribit, Binance, Binance Futures, FTX, OKEx, Huobi Global, Huobi DM, bitFlyer, Bitstamp, Coinbase Pro, Kraken Futures, Gemini, Kraken, Bitfinex, Bybit, OKCoin, CoinFLEX and more</paragraph><paragraph>automatic closed connections and stale connections reconnection logic for real-time streams</paragraph><paragraph>computing derived data locally like order book imbalance, customizable trade bars, book snapshots and more via compute helper function and computables, e.g., volume based bars, top 20 levels order book snapshots taken every 10 ms etc</paragraph><paragraph>fast and lightweight architecture — low memory footprint and no heavy in-memory buffering</paragraph><paragraph>built-in TypeScript support</paragraph><paragraph>Requires Node.js v12+ installed.</paragraph><paragraph>Debugging and logging</paragraph><paragraph>tardis-dev lib uses debugarrow-up-right package for verbose logging and debugging purposes that can be enabled via DEBUG environment variable set to tardis-dev*.</paragraph><paragraph>Usage with TypeScript</paragraph><paragraph>Simply change from require</paragraph><paragraph>to ES Modules import</paragraph><paragraph>to enjoy first class TypeScript typings.</paragraph><paragraph>Replaying historical market data</paragraph><paragraph>replay(options)</paragraph><paragraph>Replays historical market data messages for given replay options in exchange-native format. Historical market data is being fetched efficiently (in parallel) from the Tardis.dev HTTP API and cached locally. Returns async iterablearrow-up-right.</paragraph><paragraph>arrow-up-right</paragraph><paragraph>{channel:string, symbols?: string[]}[]</paragraph><paragraph>optional filters of requested historical data feed - use getExchangeDetails function to get allowed channels and symbols ids for requested exchange</paragraph><paragraph>when set to true returns messages as buffers instead of decoding them to objects</paragraph><paragraph>when set to true returns message with value undefined for events when connection that was recording the historical data got disconnected</paragraph><paragraph>API key for Tardis.dev HTTP API - if not provided only first day of each month of historical data is accessible. It can also be set via init function for all replay calls.</paragraph><paragraph>replayNormalized(options, ...normalizers)</paragraph><paragraph>Replays historical market data messages for given replay options and normalizes messages using normalizers provided as rest argumentsarrow-up-right. Historical market data is being fetched efficiently (in parallel) from the Tardis.dev HTTP API and cached locally. Returns async iterablearrow-up-right.</paragraph><paragraph>arrow-up-right</paragraph><paragraph>replay normalized options</paragraph><paragraph>optional symbols for requested data feed - use getExchangeDetails function to get allowed symbols ids for requested exchange</paragraph><paragraph>when set to true returns disconnect messages for events when connection that was recording the historical data got disconnected</paragraph><paragraph>API key for Tardis.dev HTTP API - if not provided only first day of each month of historical data is accessible. It can also be set via init function for all replayNormalized calls.</paragraph><paragraph>Built-in normalizers</paragraph><paragraph>replayNormalized function accepts any number of normalizers as rest parametersarrow-up-right that map from exchange-native format to normalized data format. tardis-dev ships with built in ones that normalize trades, order book and derivative ticker data but also allows adding custom ones.</paragraph><paragraph>Message types and formats depend on specific normalizers provided to replayNormalized function and are documented in detail in data normalization section.</paragraph><paragraph>Sample message produced by normalizeBookChanges</paragraph><paragraph>Streaming real-time market data</paragraph><paragraph>stream(options)</paragraph><paragraph>Streams real-time market data messages for given stream options in exchange-native format. It connects directly to exchanges WebSocket APIs and transparently restarts closed, broken or stale connections (open connections without data being send for specified amount of time). Returns async iterablearrow-up-right.</paragraph><paragraph>arrow-up-right</paragraph><paragraph>{channel:string, symbols?: string[]}[]</paragraph><paragraph>optional filters of requested real-time data feed - use getExchangeDetails to get allowed channels and symbols ids for requested exchange</paragraph><paragraph>when set to true returns messages as buffers instead of decoding them to objects</paragraph><paragraph>when set to true returns message with value undefined for real-time stream disconnect events</paragraph><paragraph>specifies time in milliseconds after which connection is restarted if no message has been received from the exchange</paragraph><paragraph>(err) => void | undefined</paragraph><paragraph>Optional callback invoked when real-time WebSocket connection error occurs, useful for custom error logging etc.</paragraph><paragraph>streamNormalized(options, ...normalizers)</paragraph><paragraph>Streams real-time market data messages for given stream options and normalizes messages using provided normalizers provided as rest argumentsarrow-up-right. It connects directly to exchanges WebSocket APIs and transparently restarts closed, broken or stale connections (open connections without data being send for specified amount of time). Returns async iterablearrow-up-right.</paragraph><paragraph>arrow-up-right</paragraph><paragraph>stream normalized options</paragraph><paragraph>instruments symbols for requested data feed</paragraph><paragraph>when set to true returns disconnect messages for real-time stream disconnect events</paragraph><paragraph>specifies time in milliseconds after which connection is restarted if no message has been received from the exchange</paragraph><paragraph>((err) => void) | undefined</paragraph><paragraph>Optional callback invoked when real-time WebSocket connection or mapping error occurs, useful for custom error logging etc.</paragraph><paragraph>Built-in normalizers</paragraph><paragraph>streamNormalized function can accept any number of custom normalizers as rest parametersarrow-up-right that map from exchange-native format to normalized data format. tardis-dev ships with built in ones that normalize trades, order book and derivative ticker data but also allows adding custom ones.</paragraph><paragraph>Message types and formats depend on specific normalizers provided to streamNormalized function and are documented in detail in data normalization section.</paragraph><paragraph>Sample message produced by normalizeTrades</paragraph><paragraph>Historical market data helpers</paragraph><paragraph>When working with market data viareplay and replayNormalized functions by default only first day of each month of historical data is available for replay as well as locally cached historical data is stored in default location on disk (OS temp dir).</paragraph><paragraph>Init function allows providing apiKey received via email after ordering historical market data access via Tardis.dev websitearrow-up-right as well as customcacheDir. ApiKey can also be provided directly via options of replay and replayNormalized functions - that overrides anything that was provided via init.</paragraph><paragraph>API key for Tardis.dev HTTP API - if not provided only first day of each month of historical data is accessible</paragraph><paragraph><os.tmpdir>/.tardis-cache</paragraph><paragraph>path to local dir that will be used as cache location - if not provided default temp dir for given OS will be used</paragraph><paragraph>getExchangeDetails(exchange)</paragraph><paragraph>Given exchange id provides exchange details (available symbols, availability dates, available channels, pricing info etc) provided by exchanges/:exchange API endpoint.</paragraph><paragraph>arrow-up-right</paragraph><paragraph>type of response returned by awaiting on getExchangeDetails</paragraph><paragraph>getApiKeyAccessInfo(apiKey?)</paragraph><paragraph>Given apiKey provided as optional parameter or provided in init function provides information about what historical data is available for it - exchanges, date ranges, symbols.</paragraph><paragraph>type of response returned by awaiting on getApiKeyAccessInfo()</paragraph><paragraph>Clears local data cache dir.</paragraph><paragraph>Data normalization</paragraph><paragraph>Data normalization allows consuming market data feeds from various exchanges in consistent format.</paragraph><paragraph>tardis-dev has following built-in normalizers that can be provided to replayNormalized or streamNormalized functions:</paragraph><paragraph>If you're interested in how exactly data is mapped from exchange-native format to normalized one, please follow code in tardis-dev GitHub repositoryarrow-up-right for each exchange and if you determined that mapping should be done differently please read "modifying built-in and adding custom normalizers" section.</paragraph><paragraph>arrow-up-right</paragraph><paragraph>normalizeTrades</paragraph><paragraph>When passed as an arg toreplayNormalized or streamNormalized function provides normalized trade data for all supported exchanges.</paragraph><paragraph>sample message</paragraph><paragraph>type definition</paragraph><paragraph>normalizeBookChanges</paragraph><paragraph>When passed as an arg toreplayNormalized or streamNormalized function provides normalized book_change data for all supported exchanges.</paragraph><paragraph>Provides initial L2 (market by price) order book snapshots (isSnapshot=true) plus incremental updates for each order book change. Please note that amount is the updated amount at that price level, not a delta. An amount of 0 indicates the price level can be removed.</paragraph><paragraph>sample message</paragraph><paragraph>type definition</paragraph><paragraph>normalizeDerivativeTickers</paragraph><paragraph>When passed as an arg toreplayNormalized or streamNormalized function provides normalized derivative_ticker data for supported exchanges that trade derivative instruments.</paragraph><paragraph>sample message</paragraph><paragraph>type definition</paragraph><paragraph>disconnect message</paragraph><paragraph>When replayNormalized or streamNormalized functions options have withDisconnectMessages flag set to true and disconnect event occurred (eg.: WebSocket connection close) then disconnect message is being returned.</paragraph><paragraph>sample message</paragraph><paragraph>type definition</paragraph><paragraph>Modifying built-in and adding custom normalizers</paragraph><paragraph>Intardis-dev data normalization is implemented via normalize factory functions provided to replayNormalized and streamNormalized functions. This design gives lots of flexibility by allowing replacing, extending and modifying built-in normalizers or adding new ones for new normalized data types without the need of forking the whole library.</paragraph><paragraph>Any normalize function provided to replayNormalized and streamNormalized functions needs to have following signature:</paragraph><paragraph>Exchange is an exchange id for which mapper object needs to be returned for, localTimestamp is a date for which mapper is created (and is created after each disconnection). In most cases localTimestamp is not necessary for anything, but in certain cases like for example exchange API change it can be used to switch to different mapping logic like using new data channel that wasn't available until certain date.</paragraph><paragraph>Returned Mapper object has following signature:</paragraph><paragraph>On every disconnection event that occurs normalize factory functions are called again to provide new Mapper objects with clean state if required (stateful mapping like BitMEX order book data that needs to persist state of mapping between price level id and price level and needs to 'reset' for each new connection). If mapper object is stateful it's required to always return new clean state object from normalize factory function or reset it's state in one way or another.</paragraph><paragraph>Normalized data returned by iterable iteratorarrow-up-right of Mapper.map method is expected to have a shape that has at least fields as described in normalized data type section below to play well with other tardis-dev functions like combine or compute.</paragraph><paragraph>normalized data type</paragraph><paragraph>Adding custom normalizeLiquidations normalizer</paragraph><paragraph>Example implementation of custom normalizeLiquidations function that normalizes liquidations data for deribit exchange. Implementations for for other exchanges are left as an exercise for the reader.</paragraph><paragraph>type of messages provided by normalizeLiquidations</paragraph><paragraph>implementation of deribitLiquidations mapper and normalizeLiquidations</paragraph><paragraph>normalizeLiquidations usage example</paragraph><paragraph>We could as well provide the same normalizeLiquidations function to streamNormalized function or use it together it with other normalizers (normalizeTrades etc.).</paragraph><paragraph>arrow-up-right</paragraph><paragraph>Changing normalizeTrades for Binance exchange</paragraph><paragraph>Let's assume that default normalization of Binance exchange trades data doesn't fit our use case and we need to use @aggTradearrow-up-right stream as source of trade data instead of used by default@tradearrow-up-right stream.</paragraph><paragraph>normalizeTradesWithBinancePatch usage example</paragraph><paragraph>arrow-up-right</paragraph><paragraph>Limit order book reconstruction</paragraph><paragraph>tardis-dev exports OrderBook class that when instantiated can process normalized book_change messages with order book snapshots and incremental updates and allows maintaining full local order book (level 2 - aggregated market-by-price) state both for real-time data as well as reconstructing historical order book state at any past point in time. It waits for first book_change message that is a snapshot (isSnaphot = true) and then applies subsequent updates to it. Single orderBook object can maintain order book state only for single symbol/instrument. It uses Red-Black treearrow-up-right data structure under the hood to efficiently maintain it's local state in sorted order.</paragraph><paragraph>historical order book reconstruction</paragraph><paragraph>maintaining order book for real-time stream</paragraph><paragraph>arrow-up-right</paragraph><paragraph>orderBook.update(bookChange)</paragraph><paragraph>Processes normalized book_change messages to update it's internal local state that maintains ordered bids and asks sets. It ignores any non snapshot book_change messages before initial snapshot is received. It should be called for every book_change message received for given symbol we'd like to reconstruct order book for.</paragraph><paragraph>orderBook.bestBid()</paragraph><paragraph>Returns book price level object for highest bid order (best bid) in order book or undefined if book doesn't have any bids (not initialized yet with initial snapshot).</paragraph><paragraph>orderBook.bestAsk()</paragraph><paragraph>Returns book price level object for lowest ask order (best ask) in order book or undefined if book doesn't have any asks (not initialized yet with initial snapshot).</paragraph><paragraph>orderBook.asks()</paragraph><paragraph>Returns iterable iteratorarrow-up-right of book price level objects for all asks available ordered from the lowest to highest ask.</paragraph><paragraph>orderBook.bids()</paragraph><paragraph>Returns iterable iteratorarrow-up-right of book price level objects for all bids available ordered from highest to lowest bid.</paragraph><paragraph>book price level type</paragraph><paragraph>Combining data streams</paragraph><paragraph>combine(...iterators)</paragraph><paragraph>Combine function given multiple async iterators combines them into single one. That allows synchronized historical market data replay and consolidated streaming of real-time data for multiple exchanges via single for await ...ofarrow-up-right loop.</paragraph><paragraph>Accepts async iterables of normalized messages as rest parametersarrow-up-right and combines them returning single async iteratable.</paragraph><paragraph>For historical data replay it combines input async iterables messages by sorting them by localTimestamp in ascending order, this allows synchronized/ordered market data replay for multiple exchanges.</paragraph><paragraph>For real-time market data streaming it combines input async iterables messages in FIFO order by using Promise.racearrow-up-right.</paragraph><paragraph>combining historical market data from multiple exchanges</paragraph><paragraph>combining real-time stream of market data from multiple exchanges</paragraph><paragraph>arrow-up-right</paragraph><paragraph>Computing derived data locally</paragraph><paragraph>compute(iterator, ...computables)</paragraph><paragraph>Compute function allows computing various derived data locally via so called computables like:</paragraph><paragraph>If you're interested in adding custom computables like for example order book imbalance, volume imbalance, open interest or funding rate based bars please read "adding custom computable" section.</paragraph><paragraph>Compute function accepts async iterable producing normalized messages together withcomputables as a rest parametersarrow-up-right and returns async iterable with normalized messages produced by provided iterable and additionally all computed messages based on provided computable functions. It computes and produces separate computed normalized messages for each symbol and exchange combination. When disconnect message is returned by provided async iterable it discards existing pending computables and starts from computing them from scratch.</paragraph><paragraph>arrow-up-right</paragraph><paragraph>computeTradeBars(options)</paragraph><paragraph>When provided to compute function, computes normalized trade_bar messages based on normalized trade data.</paragraph><paragraph>compute trade bars options</paragraph><paragraph>| 'time'</paragraph><paragraph>| 'volume'</paragraph><paragraph>| 'tick'</paragraph><paragraph>determines the way trades within a bar will be aggregated.</paragraph><paragraph>time - classic OHLC candles aggregated by time</paragraph><paragraph>volume - volume based trade bars agg by sum of trades amount tick - tick based trade bars, aggregated by trades count</paragraph><paragraph>determines interval to aggregate by - for time based bars it's number of milliseconds, for volume based bars it's accumulated volume, for tick it's count of trades</paragraph><paragraph>optional custom name of trade_bar, if not specified computed name will be provided based on kind and interval options</paragraph><paragraph>type of message provided by computeTradeBars</paragraph><paragraph>sample normalized trade_bar message</paragraph><paragraph>computeBookSnapshots(options)</paragraph><paragraph>When provided to compute function, computes normalized book_snapshot messages based on normalized order book data. It produces new snapshots only if there is an actual change in order book state for requested depth.</paragraph><paragraph>compute book snapshots options</paragraph><paragraph>number of closest bids and asks levels to provide snaphot for</paragraph><paragraph>snapshot interval in milliseconds, if 0 is provided it computes snapshots real-time any time there is a change in order book state for requested depth</paragraph><paragraph>optional custom name of book_snapshot, if not specified computed name will be provided based on depth and interval options</paragraph><paragraph>type of message provided by computeBookSnaphots</paragraph><paragraph>sample normalized book_snapshot message</paragraph><paragraph>Adding custom computable</paragraph><paragraph>Any computables provided to compute function need to be a factory functions with following signature:</paragraph><paragraph>where returned Computable object has following signature:</paragraph><paragraph>Computable.compute returned iterator is expected to provide objects that at least have fields as described in normalized data type section to play well with other tardis-dev functions like combine.</paragraph><paragraph>computeOrderBookImbalanceRatio()</paragraph><paragraph>Example implementation of custom computeOrderBookImbalanceRatio function that as a source data type uses book snapshots and based on it computes ratio of asks amounts (sell orders) to bids amounts (buy orders) for given book_snapshot depth. It may be used to determine relative buy or sell pressure.</paragraph><paragraph>type of messages produced by computeOrderBookImbalanceRatio</paragraph><paragraph>implementation of BookImbalanceRatioComputable computable and computeOrderBookImbalanceRatio factory function.</paragraph><paragraph>computeOrderBookImbalanceRatio usage example</paragraph><paragraph>Given implementation above we can compute book imbalance ratio for BitMEX real-time XBTUSD message stream. For this example we compute top 5 levels, 2 second book snapshots as a source to our custom computable. We need to have async iterable that produces book snapshots as a source to our book imbalance computable, hence two invocations of compute.</paragraph><paragraph>arrow-up-right</paragraph><paragraph>Real-time spread across multiple exchanges</paragraph><paragraph>Example showing how to very easy display real-time spread and best bid/ask info across multiple exchanges at once. It can be easily adapted to do the same for historical data (replayNormalized instead of streamNormalized).</paragraph><paragraph>arrow-up-right</paragraph><paragraph>Replay large historical trades across multiple exchanges</paragraph><paragraph>Example showing replaying large historical trades across multiple exchanges as those happened.</paragraph><paragraph>arrow-up-right</paragraph><paragraph>Seamless switching between real-time streaming and historical market data replay</paragraph><paragraph>Example showing simple pattern of providing async iterable of market data messages to the function that can process them no matter if it's is real-time or historical market data. That effectively enables having the same 'data pipeline' for backtesting and live trading.</paragraph><paragraph>arrow-up-right</paragraph><paragraph>Real-time funding rate and open interest across multiple exchanges</paragraph><paragraph>Example showing how to quickly display real-time funding rate and open interest info across multiple exchanges at once.</paragraph><paragraph>arrow-up-right</paragraph><paragraph>Saving historical funding, index and open interest data to CSV file</paragraph><paragraph>Example showing how to write Deribit exchange historical funding, index and open interest data into CSV.</paragraph><paragraph>arrow-up-right</paragraph><paragraph>Computing simple moving average of volume based trade bars</paragraph><paragraph>Example showing implementation of SimpleMovingAverageComputable that calculates average of trade bar closes prices for specified rolling window in incremental way. It uses CircularBuffer under the hood.</paragraph><paragraph>arrow-up-right</paragraph></content>
</page>
<page url="https://docs.tardis.dev/api/tardis-machine">
<title>Tardis Machine Server | Tardis.dev Documentation</title>
<content><paragraph>Tardis-machinearrow-up-right is a locally runnable server with built-in data caching that uses Tardis.dev HTTP API under the hood. It provides both tick-level historical and consolidated real-time cryptocurrency market data via it's HTTP and WebSocket APIs and is available via npm and Docker.</paragraph><list type="ul"><item>efficient data replay API endpoints returning historical market data for whole time periods (in contrast to Tardis.dev HTTP API where single call returns data for single minute time period)</item><item>exchange-native market data APIsWebSocket API providing historical market data replay from any given past point in time with the same data format and 'subscribe' logic as real-time exchanges' APIs - in many cases existing exchanges' WebSocket clients can be used to connect to this endpoint</item><item>transparent historical local data caching (cached data is stored on disk in compressed GZIP format and decompressed on demand when reading the data)</item><item>support for top cryptocurrency exchanges: BitMEX, Deribit, Binance, Binance Futures, FTX, OKEx, Huobi Global, Huobi DM, bitFlyer, Bitstamp, Coinbase Pro, Kraken Futures, Gemini, Kraken, Bitfinex, Bybit, OKCoin, CoinFLEX and more</item></list><paragraph>efficient data replay API endpoints returning historical market data for whole time periods (in contrast to Tardis.dev HTTP API where single call returns data for single minute time period)</paragraph><paragraph>exchange-native market data APIs</paragraph><list type="ul"><item>WebSocket API providing historical market data replay from any given past point in time with the same data format and 'subscribe' logic as real-time exchanges' APIs - in many cases existing exchanges' WebSocket clients can be used to connect to this endpoint</item></list><paragraph>WebSocket API providing historical market data replay from any given past point in time with the same data format and 'subscribe' logic as real-time exchanges' APIs - in many cases existing exchanges' WebSocket clients can be used to connect to this endpoint</paragraph><paragraph>transparent historical local data caching (cached data is stored on disk in compressed GZIP format and decompressed on demand when reading the data)</paragraph><paragraph>support for top cryptocurrency exchanges: BitMEX, Deribit, Binance, Binance Futures, FTX, OKEx, Huobi Global, Huobi DM, bitFlyer, Bitstamp, Coinbase Pro, Kraken Futures, Gemini, Kraken, Bitfinex, Bybit, OKCoin, CoinFLEX and more</paragraph><paragraph>Pull and run latest version of tardisdev/tardis-machine imagearrow-up-right:</paragraph><paragraph>Tardis-machine server's HTTP endpoints will be available on port 8000 and WebSocket API endpoints on port 8001. Your API key will be passed via ENV variable (TM_API_KEY) — simply replace YOUR_API_KEY with API key you've received via email.</paragraph><paragraph>Command above does not use persistent volumes for local caching (each docker restart will result in loosing local data cache). In order to use for example./host-cache-dir as persistent volume (bind mountarrow-up-right) cache directory, run:</paragraph><paragraph>Since using volumes can cause issues especially on Windows, it's fine to run Docker image without them with the caveat of potentially poor local cache ratio after each container's restart.</paragraph><paragraph>Config environment variables</paragraph><paragraph>You can set following environment config variables to configure tardis-machine server:</paragraph><paragraph>API key for Tardis.devarrow-up-right HTTP API - if not provided only first day of each month of historical data is accessible</paragraph><paragraph>HTTP port on which server will be running, WebSocket port is always this value + 1 (8001 with port set to 8000)</paragraph><paragraph>path to local dir that will be used as cache location</paragraph><paragraph>server will print verbose debug logs to stdout if set to true</paragraph><paragraph>server will clear local cache dir on startup if set to true</paragraph><paragraph>Install and runtardis-machine server via npx command:</paragraph><paragraph>or install globally via npm:</paragraph><paragraph>and then run:</paragraph><paragraph>Tardis-machine server's HTTP endpoints will be available on port 8000 and WebSocket API endpoints on port 8001. Your API key will be passed via --api-key config flag — simply replace YOUR_API_KEY with API key you've received via email.</paragraph><paragraph>CLI config flags</paragraph><paragraph>You can set following CLI config flags when starting tardis-machine server installed via npm:</paragraph><paragraph>API key for Tardis.devarrow-up-right HTTP API - if not provided only first day of each month of historical data is accessible</paragraph><paragraph>HTTP port on which server will be running, WebSocket port is always this value + 1 (8001 with port set to 8000)</paragraph><paragraph><os.tmpdir>/.tardis-cache</paragraph><paragraph>path to local dir that will be used as cache location - if not provided default temp dir for given OS will be used</paragraph><paragraph>server will print verbose debug logs to stdout if set to true</paragraph><paragraph>server will clear local cache dir on startup is set to true</paragraph><paragraph>shows tardis-machine version number</paragraph><paragraph>Exchange-native market data APIs</paragraph><paragraph>Exchange-native market data API endpoints provide historical data in exchange-native format. The main difference between HTTP and WebSocket endpoints is the logic of requesting data:</paragraph><list type="ul"><item>WebSocket API accepts exchanges' specific 'subscribe' messages that define what data will be then "replayed" and send to WebSocket client</item></list><paragraph>WebSocket API accepts exchanges' specific 'subscribe' messages that define what data will be then "replayed" and send to WebSocket client</paragraph><paragraph>• HTTP GET /replay?options={options}</paragraph><paragraph>Returns historical market data messages in exchange-native format for given replay options query string param. Single streaming HTTP response returns data for the whole requested time period as NDJSONarrow-up-right.</paragraph><paragraph>Python</paragraph><paragraph>Node.js</paragraph><paragraph>cURL</paragraph><paragraph>Your preferred language</paragraph><paragraph>We're working on providing more samples and dedicated client libraries in different languages, but in the meanwhile to consume HTTP /replay API responses in your language of choice, you should:</paragraph><list type="ol"><item>Provide url encoded JSON options object via options query string param when sending HTTP request</item><item>Parse HTTP response stream line by line as it's returned - buffering in memory whole response may result in slow performance and memory overflows</item></list><paragraph>Provide url encoded JSON options object via options query string param when sending HTTP request</paragraph><paragraph>Parse HTTP response stream line by line as it's returned - buffering in memory whole response may result in slow performance and memory overflows</paragraph><paragraph>HTTP /replay endpoint accepts required options query string param in url encoded JSON format.</paragraph><paragraph>{channel:string, symbols?: string[]}[]</paragraph><paragraph>when set to true, response includes empty lines (\n) that mark events when real-time WebSocket connection that was used to collect the historical data got disconnected</paragraph><paragraph>Response format</paragraph><paragraph>Streamed HTTP response provides data in NDJSON format (new line delimited JSON) - each response line is a JSON with market data message in exchange-native format plus local timestamp:</paragraph><list type="ul"><item>localTimestamp - date when message has been received in ISO 8601 format</item><item>message - JSON with exactly the same format as provided by requested exchange real-time feeds</item></list><paragraph>localTimestamp - date when message has been received in ISO 8601 format</paragraph><paragraph>message - JSON with exactly the same format as provided by requested exchange real-time feeds</paragraph><paragraph>Sample response</paragraph><paragraph>• WebSocket /ws-replay?exchange={exchange}&from={fromDate}&to={toDate}</paragraph><paragraph>Exchanges' WebSocket APIs are designed to publish real-time market data feeds, not historical ones. Tardis-machine WebSocket /ws-replay API fills that gap and allows "replaying" historical market data from any given past point in time with the same data format and 'subscribe' logic as real-time exchanges' APIs. In many cases existing exchanges' WebSocket clients can be used to connect to this endpoint just by changing URL, and receive historical market data in exchange-native format for date ranges specified in URL query string params.</paragraph><paragraph>After connection is established, client has 2 seconds to send subscriptions payloads and then market data replay starts.</paragraph><paragraph>If two clients connect at the same time requesting data for different exchanges and provide the same session key via query string param, then data being send to those clients will be synchronized (by local timestamp).</paragraph><paragraph>Python</paragraph><paragraph>Node.js</paragraph><paragraph>Your preferred language</paragraph><paragraph>As long as you already use existing WebSocket client that connects to and consumes real-time exchange market data feed, in most cases you can use it to connect to /ws-replay API as well just by changing URL endpoint.</paragraph><paragraph>Query string params</paragraph><paragraph>optional replay session key. When specified and multiple clients use it when connecting at the same time then data being send to those clients is synchronized (by local timestamp).</paragraph><paragraph>Normalized market data APIs</paragraph><paragraph>Normalized market data API endpoints provide data in unified format across all supported exchanges. Both HTTP /replay-normalized and WebSocket /ws-replay-normalized APIs accept the same replay options payload via query string param. It's mostly matter of preference when choosing which protocol to use, but WebSocket /ws-replay-normalized API has also it's real-time counterpart /ws-stream-normalized, which connects directly to exchanges' real-time WebSocket APIs. This opens the possibility of seamless switching between real-time streaming and historical normalized market data replay.</paragraph><paragraph>• HTTP GET /replay-normalized?options={options}</paragraph><paragraph>Returns historical market data for data types specified via query string. Single streaming HTTP response returns data for the whole requested time period as NDJSONarrow-up-right. See supported data types which include normalized trade, order book change, customizable order book snapshots etc.</paragraph><paragraph>Python</paragraph><paragraph>Node.js</paragraph><paragraph>cURL</paragraph><paragraph>Your preferred language</paragraph><paragraph>We're working on providing more samples and dedicated client libraries in different languages, but in the meanwhile to consume HTTP /replay-normalized API responses in your language of choice, you should:</paragraph><list type="ol"><item>Provide url encoded JSON options via options query string param when sending HTTP request</item><item>Parse HTTP response stream line by line as it's returned - buffering in memory whole response may result in slow performance and memory overflows</item></list><paragraph>Provide url encoded JSON options via options query string param when sending HTTP request</paragraph><paragraph>Parse HTTP response stream line by line as it's returned - buffering in memory whole response may result in slow performance and memory overflows</paragraph><paragraph>Replay normalized options</paragraph><paragraph>HTTP /replay-normalized endpoint accepts required options query string param in url encoded JSON format.</paragraph><paragraph>Options JSON needs to be an object or an array of objects with fields as specified below. If array is provided, then data requested for multiple exchanges is returned synchronized (by local timestamp).</paragraph><paragraph>array of normalized data types for which historical data will be returned</paragraph><paragraph>when set to true, response includes disconnect messages that mark events when real-time WebSocket connection that was used to collect the historical data got disconnected</paragraph><paragraph>Response format & sample messages</paragraph><paragraph>See normalized data types.</paragraph><paragraph>• WebSocket /ws-replay-normalized?options={options}</paragraph><paragraph>Sends normalized historical market data for data types specified via query string. See supported data types which include normalized trade, order book change, customizable order book snapshots etc.</paragraph><paragraph>Python</paragraph><paragraph>Node.js</paragraph><paragraph>Your preferred language</paragraph><paragraph>We're working on providing more samples and dedicated client libraries in different languages, but in the meanwhile to consume WebSocket /ws-replay-normalized API responses in your language of choice, you should:</paragraph><paragraph>Replay normalized options</paragraph><paragraph>WebSocket /ws-replay-normalized endpoint accepts required options query string param in url encoded JSON format.</paragraph><paragraph>Options JSON needs to be an object or an array of objects with fields as specified below. If array is provided, then data requested for multiple exchanges is being send synchronized (by local timestamp).</paragraph><paragraph>array of normalized data types for which historical data will be provided</paragraph><paragraph>when set to true, sends also disconnect messages that mark events when real-time WebSocket connection that was used to collect the historical data got disconnected</paragraph><paragraph>Response format & sample messages</paragraph><paragraph>See normalized data types.</paragraph><paragraph>• WebSocket /ws-stream-normalized?options={options}</paragraph><paragraph>Sends normalized real-time market data for data types specified via query string. See supported data types which include normalized trade, order book change, customizable order book snapshots etc.</paragraph><paragraph>Doesn't requires API key as connects directly to exchanges real-time WebSocket APIs and transparently restarts closed, broken or stale connections (open connections without data being send for specified amount of time).</paragraph><paragraph>Provides consolidated real-time market data streaming functionality with options as an array - provides single consolidated real-time data stream for all exchanges specified in options array.</paragraph><paragraph>Python</paragraph><paragraph>Node.js</paragraph><paragraph>Your preferred language</paragraph><paragraph>We're working on providing more samples and dedicated client libraries in different languages, but in the meanwhile to consume WebSocket /ws-stream-normalized API responses in your language of choice, you should:</paragraph><paragraph>Stream normalized options</paragraph><paragraph>WebSocket /ws-stream-normalized endpoint accepts required options query string param in url encoded JSON format.</paragraph><paragraph>Options JSON needs to be an object or an array of objects with fields as specified below. If array is specified then API provides single consolidated real-time data stream for all exchanges specified (as in examples above).</paragraph><paragraph>optional symbols of requested real-time data feed</paragraph><paragraph>array of normalized data types for which real-time data will be provided</paragraph><paragraph>when set to true, sends disconnect messages anytime underlying exchange real-time WebSocket connection(s) gets disconnected</paragraph><paragraph>specifies time in milliseconds after which connection to real-time exchanges' WebSocket API is restarted if no message has been received</paragraph><paragraph>Response format & sample messages</paragraph><paragraph>See normalized data types.</paragraph><paragraph>Normalized data types</paragraph><paragraph>Individual trade</paragraph><paragraph>Initial L2 (market by price) order book snapshot (isSnapshot=true) plus incremental updates for each order book change. Please note that amount is the updated amount at that price level, not a delta. An amount of 0 indicates the price level can be removed.</paragraph><paragraph>• derivative_ticker</paragraph><paragraph>Derivative instrument ticker info sourced from real-time ticker & instrument channels.</paragraph><paragraph>• book_snapshot_{number_of_levels}_{snapshot_interval}{time_unit}</paragraph><paragraph>Order book snapshot for selected number_of_levels (top bids and asks), snapshot_interval and time_unit. When snapshot_interval is set to 0 , snapshots are taken anytime order book state within specified levels has changed, otherwise snapshots are taken anytime snapshot_interval time has passed and there was an order book state change within specified levels. Order book snapshots are computed from exchanges' real-time order book streaming L2 data (market by price).</paragraph><list type="ul"><item>book_snapshot_10_0ms - provides top 10 levels tick-by-tick order book snapshots</item><item>book_snapshot_50_100ms - provides top 50 levels order book snapshots taken at 100 millisecond intervals</item><item>book_snapshot_30_10s - provides top 30 levels order book snapshots taken at 10 second intervals</item><item>quote is an alias of book_snapshot_1_0ms - provides top of the book (best bid/ask) tick-by-order book snapshots</item><item>quote_10s is an alias of book_snapshot_1_10s - provides top of the book (best bid/ask) order book snapshots taken at 10 seconds intervals</item></list><paragraph>book_snapshot_10_0ms - provides top 10 levels tick-by-tick order book snapshots</paragraph><paragraph>book_snapshot_50_100ms - provides top 50 levels order book snapshots taken at 100 millisecond intervals</paragraph><paragraph>book_snapshot_30_10s - provides top 30 levels order book snapshots taken at 10 second intervals</paragraph><paragraph>quote is an alias of book_snapshot_1_0ms - provides top of the book (best bid/ask) tick-by-order book snapshots</paragraph><paragraph>quote_10s is an alias of book_snapshot_1_10s - provides top of the book (best bid/ask) order book snapshots taken at 10 seconds intervals</paragraph><paragraph>Available time units:</paragraph><paragraph>• trade_bar_{aggregation_interval}{suffix}</paragraph><paragraph>Trades data in aggregated form, known as OHLC, candlesticks, klines etc. Not only most common time based aggregation is supported, but volume and tick count based as well. Bars are computed from tick-by-tick raw trade data, if in given interval no trades happened, there is no bar produced.</paragraph><list type="ul"><item>trade_bar_10ms - provides time based trade bars with 10 milliseconds intervals</item><item>trade_bar_5m - provides time based trade bars with 5 minute intervals</item><item>trade_bar_100ticks - provides ticks based trade bars with 100 ticks (individual trades) intervals</item><item>trade_bar_100000vol - provides volume based trade bars with 100 000 volume intervals</item></list><paragraph>trade_bar_10ms - provides time based trade bars with 10 milliseconds intervals</paragraph><paragraph>trade_bar_5m - provides time based trade bars with 5 minute intervals</paragraph><paragraph>trade_bar_100ticks - provides ticks based trade bars with 100 ticks (individual trades) intervals</paragraph><paragraph>trade_bar_100000vol - provides volume based trade bars with 100 000 volume intervals</paragraph><paragraph>Allowed suffixes:</paragraph><paragraph>Message that marks events when real-time WebSocket connection that was used to collect the historical data got disconnected.</paragraph></content>
</page>
<page url="https://docs.tardis.dev/api/instruments-metadata-api">
<title>Instruments Metadata API | Tardis.dev Documentation</title>
<content><paragraph>Please provide 'Authorization' header with value: 'Bearer YOUR_API_KEY'.</paragraph><paragraph>Returns instrument info for provided exchange and symbol.</paragraph><code language="javascript">{
	id: string // symbol id
	datasetId: string // id used for CSV datasets downloads
	exchange: string // exchange id
	baseCurrency: string // normalized, so for example bitmex XBTUSD has base currency set to BTC not XBT
	quoteCurrency: string // normalized, so for example bitfinex BTCUST has quote currency set to USDT, not UST
	type: 'spot' | 'perpetual' | 'future' | 'option'
	active: boolean // indicates if the instrument can currently be traded.
	availableSince: string, // date in ISO format
	availableTo: string | undefined, // date in ISO format
	expiry: string | undefined // in ISO format, only for futures and options 
	priceIncrement: number  // price tick size, price precision can be calculated from it
	amountIncrement: number  // amount tick size, amount/size precision can be calculated from it
	minTradeAmount: number // min order size
	margin: boolean | undefined // set to true for margin enabled spot currency pairs, only available for spot symbols
	makerFee: number // consider it  as illustrative only, as it depends in practice on account traded volume levels, different categories, VIP levels, owning exchange currency etc
	takerFee: number // consider it  as illustrative only, as it depends in practice on account traded volume levels, different categories, VIP levels, owning exchange currency etc
	inverse: boolean | undefined // only for derivatives
	contractType: "move" | "linear_future" | "inverse_future" | "quanto_future" | "linear_perpetual" | "inverse_perpetual" | "quanto_perpetual" | "put_option" | "call_option" | "turbo_put_option" | "turbo_call_option" | "spread" | "interest_rate_swap" | "repo" | "index" // only for derivatives, detailed contract type
	contractMultiplier: number | undefined // only for derivatives
	quanto: boolean | undefined // set to true for quanto instruments, otherwise undefined
  	settlementCurrency: string | undefined // settlement currency, only for quanto instruments as it's different base/quote currency
	strikePrice: number | undefined // strike price, only for options
        optionType: 'call' | 'put' // option type, only for options
	
	changes: undefined | {
		until: string // date in ISO format
		priceIncrement: number | undefined
		amountIncrement: number | undefined
		contractMultiplier: number | undefined
	}[]
} </code><paragraph>JSON object, when provided via query string it needs be url encoded.</paragraph></content>
</page>
<page url="https://docs.tardis.dev/api/http">
<title>HTTP API Reference | Tardis.dev Documentation</title>
<content><code language="javascript">[
  {
    "id": "bitmex",
    "name": "BitMEX",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-03-30T00:00:00.000Z"
  },
  {
    "id": "deribit",
    "name": "Deribit",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-03-30T00:00:00.000Z"
  },
  {
    "id": "binance-futures",
    "name": "Binance Futures",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-11-17T00:00:00.000Z"
  },
  {
    "id": "binance",
    "name": "Binance (high caps)",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-03-30T00:00:00.000Z"
  },
  {
    "id": "ftx",
    "name": "FTX",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-08-01T00:00:00.000Z"
  },
  {
    "id": "okex-futures",
    "name": "OKEx Futures",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-03-30T00:00:00.000Z"
  },
  {
    "id": "okex-options",
    "name": "OKEx Options",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2020-02-01T00:00:00.000Z"
  },
  {
    "id": "okex-swap",
    "name": "OKEx Perpetual Swap",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-03-30T00:00:00.000Z"
  },
  {
    "id": "okex",
    "name": "OKEx Spot (high caps)",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-03-30T00:00:00.000Z"
  },
  {
    "id": "huobi-dm",
    "name": "Huobi DM",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-11-19T00:00:00.000Z"
  },
  {
    "id": "huobi-dm-swap",
    "name": "Huobi DM Swap",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2020-03-28T00:00:00.000Z"
  },
  {
    "id": "huobi",
    "name": "Huobi Spot (high caps)",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-11-19T00:00:00.000Z"
  },
  {
    "id": "bitfinex-derivatives",
    "name": "Bitfinex Derivatives",
    "enabled": true,
    "filterable": false,
    "supportsDatasets": true,
    "availableSince": "2019-09-14T00:00:00.000Z"
  },
  {
    "id": "bitfinex",
    "name": "Bitfinex (high caps)",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-05-23T00:00:00.000Z"
  },
  {
    "id": "bitfinex-alts",
    "name": "Bitfinex (alts)",
    "enabled": true,
    "filterable": false,
    "supportsDatasets": true,
    "availableSince": "2020-02-01T00:00:00.000Z"
  },
  {
    "id": "bitflyer",
    "name": "bitFlyer",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-08-30T00:00:00.000Z"
  },
  {
    "id": "cryptofacilities",
    "name": "Kraken Futures (Crypto Facilities)",
    "enabled": false,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-03-30T00:00:00.000Z"
  },
  {
    "id": "kraken",
    "name": "Kraken",
    "enabled": false,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-06-04T00:00:00.000Z"
  },
  {
    "id": "bitstamp",
    "name": "Bitstamp",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-03-30T00:00:00.000Z"
  },
  {
    "id": "coinbase",
    "name": "Coinbase Pro",
    "enabled": false,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-03-30T00:00:00.000Z"
  },
  {
    "id": "gemini",
    "name": "Gemini",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-08-30T00:00:00.000Z"
  },
  {
    "id": "coinflex",
    "name": "CoinFLEX",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": false,
    "availableSince": "2020-02-01T00:00:00.000Z"
  },
  {
    "id": "bybit",
    "name": "Bybit",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-11-07T00:00:00.000Z"
  },
  {
    "id": "phemex",
    "name": "Phemex",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2020-03-17T00:00:00.000Z"
  },
  {
    "id": "okcoin",
    "name": "OKCoin",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-11-19T00:00:00.000Z"
  },
  {
    "id": "hitbtc",
    "name": "HitBTC (high caps)",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": false,
    "availableSince": "2019-11-19T00:00:00.000Z"
  },
  {
    "id": "binance-jersey",
    "name": "Binance Jersey",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-10-30T00:00:00.000Z"
  },
  {
    "id": "binance-us",
    "name": "Binance US",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": true,
    "availableSince": "2019-09-25T00:00:00.000Z"
  },
  {
    "id": "binance-dex",
    "name": "Binance DEX",
    "enabled": true,
    "filterable": true,
    "supportsDatasets": false,
    "availableSince": "2019-06-04T00:00:00.000Z"
  }
]</code></content>
</page>
<page url="https://docs.tardis.dev/legal/privacy-policy">
<title>Privacy Policy | Tardis.dev Documentation</title>
<content><paragraph>This Privacy Policy explains how Personal Information about our (potential) customers and other individuals using our services is collected, used and disclosed by Tardis.dev and its respective affiliates ("us", "we", "our" or "Tardis.dev"). This Privacy Policy describes our privacy practices in relation to the use of our websites (including any customer portal or interactive customer website) (https://tardis.devarrow-up-right and https://docs.tardis.devarrow-up-right), services, solutions, tools, and related applications, services, and programs, including research and marketing activities, offered by us (the "Services"), as well as your choices regarding use, access, storage and correction of Personal Information. It also describes how we collect, use, disclose and otherwise process Personal Information collected in relation to our Services and otherwise in the course of our business activities. This Privacy Policy does not apply to Personal Information collected about our employees, applicants or other personnel.</paragraph><paragraph>By using our Services or by agreeing to our Terms of Service required to use our Services, you agree to the collection, usage, storage and disclosure of information described in this Privacy Policy.</paragraph><paragraph>Our Services may contain links to other websites or services; and information practices and/or the content of such other websites or services shall be governed by the privacy statements of such other websites or services.</paragraph><paragraph>What information do we collect?</paragraph><paragraph>Here we describe what information we collect, what we use it for, which third parties we use to help us and give links to the privacy policies of those third parties for you to read:</paragraph><list type="ul"><item>we track and store usage behavior such as which links are clicked on and API endpoints usage statistics so we can optimize the services we provide to you. We use Google Analytics with anonymized IP feature enabled for this purpose. Please refer to their Privacy Statement https://www.google.com/policies/privacy/arrow-up-right</item><item>When you contact us for support or other customer service requests, we can maintain records related to such requests, including any information provided by you related to such support or service requests and contact you back about our services with relevant information. We may also obtain Personal Information about you from third parties, such as LinkedIn, Facebook, Twitter and other publicly accessible sources.</item><item>We may use your Personal Information to contact you with marketing or promotional materials and other information communications related to Tardis.dev. If you no longer wish to receive marketing or promotional communications related to us, you can at any moment in time by using the unsubscribe button in the email or emailing [email protected]envelope to request us to stop sending you such communications. Such a request will be processed immediately by us, but in any event within two (2) business days.</item><item>To keep track of which users should have access to paid versions of our services and to handle API authentication and authorization, we store user details such as email address, name, subscription details and IP address in Cloudflare data store (encrypted at rest). Please refer to their Privacy Statement https://www.cloudflare.com/privacypolicyarrow-up-right</item></list><paragraph>we track and store usage behavior such as which links are clicked on and API endpoints usage statistics so we can optimize the services we provide to you. We use Google Analytics with anonymized IP feature enabled for this purpose. Please refer to their Privacy Statement https://www.google.com/policies/privacy/arrow-up-right</paragraph><paragraph>When you contact us for support or other customer service requests, we can maintain records related to such requests, including any information provided by you related to such support or service requests and contact you back about our services with relevant information. We may also obtain Personal Information about you from third parties, such as LinkedIn, Facebook, Twitter and other publicly accessible sources.</paragraph><paragraph>We may use your Personal Information to contact you with marketing or promotional materials and other information communications related to Tardis.dev. If you no longer wish to receive marketing or promotional communications related to us, you can at any moment in time by using the unsubscribe button in the email or emailing [email protected]envelope to request us to stop sending you such communications. Such a request will be processed immediately by us, but in any event within two (2) business days.</paragraph><paragraph>To keep track of which users should have access to paid versions of our services and to handle API authentication and authorization, we store user details such as email address, name, subscription details and IP address in Cloudflare data store (encrypted at rest). Please refer to their Privacy Statement https://www.cloudflare.com/privacypolicyarrow-up-right</paragraph><paragraph>Unless specified otherwise, all data requested by Tardis.dev is mandatory and failure to provide this data may make it impossible for us to provide our services. In cases where Tardis.dev website specifically states that some data is not mandatory, users are free not to communicate this data without consequences to the availability or the functioning of the service. Users who are uncertain about which personal data is mandatory are welcome to contact us at [email protected]envelope.</paragraph><paragraph>We may publicly display aggregated anonymous data to help communicate what we know about how our services are typically used.</paragraph><paragraph>How do you get my consent?</paragraph><paragraph>When you provide us with personal information to make a purchase or return a purchase, we imply that you consent to us collecting this information and using it for that specific reason only.</paragraph><paragraph>If we ask for your personal information so that we can send you communications in the future, we will either ask you directly for your expressed consent or provide you with an opportunity to say no afterwards.</paragraph><paragraph>How do I withdraw my consent?</paragraph><paragraph>If you wish to withdraw the consent you have given to us to collect, store or use your information, please contact us at [email protected]envelope.</paragraph><paragraph>We may disclose your personal information if we are required by law to do so or if you violate our Terms of Service.</paragraph><paragraph>Third-party services</paragraph><paragraph>Third-party providers will collect, use and disclose your information to the extent necessary to allow them to perform the services they provide to us. Third-party service providers have their own privacy policies in respect to the information we are required to provide to them. For these providers, we recommend that you read their privacy policies so you can understand the manner in which your personal information will be handled by these providers.</paragraph><paragraph>In particular, remember that certain providers may be located in or have facilities that are located in a different jurisdiction than either you or us. Your information may become subject to the laws of the jurisdiction(s) in which a third party service provider or its facilities are located.</paragraph><paragraph>When you click on links that appear in any of the content we provide to you, those links may direct you to third party sites. We are not responsible for the privacy practices of other sites and encourage you to read their privacy statements.</paragraph><paragraph>To protect your personal information, we take reasonable precautions to make sure it is not inappropriately lost, misused, accessed, disclosed, altered or destroyed.</paragraph><paragraph>Our website uses cookiesarrow-up-right to track anonymized usage behavior and to personalize content.</paragraph><paragraph>Retention of Personal Information</paragraph><paragraph>We retain personal information that you provide us as long as we consider it potentially useful in contacting you about our services, or as needed to comply with our legal obligations, resolve disputes and enforce our agreements.</paragraph><paragraph>By using Tardis.dev website and services, you represent that you are at least the age of majority in your state or province of residence, or that you are the age of majority in your state or province of residence and you have given us your consent to allow any of your minor dependents to use this site.</paragraph><paragraph>Changes to this privacy policy</paragraph><paragraph>We reserve the right to modify this privacy policy at any time, so please review it frequently. Changes and clarifications will take effect immediately upon their posting on the website. If we make material changes to this policy, we will notify you here that it has been updated, so that you are aware of what information we collect, how we use it, and under what circumstances, if any, we use and/or disclose it.</paragraph><paragraph>If we are acquired or merged with another company, your information may be transferred to the new owners.</paragraph><paragraph>If you would like to access, correct, amend or delete any personal information we have about you, register a complaint, or simply want more information you should email us at [email protected]envelope.</paragraph></content>
</page>
<page url="https://docs.tardis.dev/legal/terms-of-service">
<title>Terms of Service | Tardis.dev Documentation</title>
<content><paragraph>Last modification date: 2023-09-12</paragraph><paragraph>General conditions</paragraph><paragraph>Throughout the page, the terms "we", "us" and "our" refer to Tardis.dev.</paragraph><paragraph>Please read these Terms of Service carefully before using our Service. By using our websites (including any customer portal or interactive customer website) (https://tardis.devarrow-up-right and https://docs.tardis.devarrow-up-right), services, solutions, tools, and related applications, services, and programs, including research and marketing activities, offered by us (the "Services"), you agree to be bound by these Terms of Service.</paragraph><paragraph>By visiting our site, purchasing something from us, accessing material we make available or using any software we provide such as API service, you engage in our "Service" and agree to be bound by the following terms and conditions ("Terms of Service", "Terms"), including those additional terms and conditions and policies referenced herein and/or available by hyperlink. These Terms of Service apply to all users of the site, including without limitation users who are browsers, vendors, customers, merchants, and/or contributors of content.</paragraph><paragraph>Any new features or tools we offer are subject to the Terms of Service. You can review the most current version of the Terms of Service at any time on this page. We reserve the right to update, change or replace any part of these Terms of Service by posting updates and/or changes to our website. It is your responsibility to check this page periodically for changes. Your continued use of our Service following the posting of any changes constitutes acceptance of those changes.</paragraph><paragraph>By agreeing to these Terms of Service, you represent that you are at least the age of majority in your state or province of residence, or that you are the age of majority in your state or province of residence and you have given us your consent to allow any of your minor dependents to use this site.</paragraph><paragraph>A breach or violation of any of the Terms will result in an immediate termination of your Services.</paragraph><paragraph>We reserve the right to refuse service to anyone for any reason at any time.</paragraph><paragraph>We reserve the right at any time to modify or discontinue the Service (or any part or content thereof) without notice at any time.</paragraph><paragraph>We shall not be liable to you or to any third-party for any modification, price change, suspension or discontinuance of the Service.</paragraph><paragraph>You agree to indemnify, defend and hold harmless Tardis.dev and our affiliates, officers, directors, contractors and employees, harmless from any claim or demand, including reasonable attorneys' fees, made by any third-party due to or arising out of your breach of these Terms of Service or the documents they incorporate by reference, or your violation of any law or the rights of a third-party.</paragraph><paragraph>In the event that any provision of these Terms of Service is determined to be unlawful, void or unenforceable, such provision shall nonetheless be enforceable to the fullest extent permitted by applicable law, and the unenforceable portion shall be deemed to be severed from these Terms of Service, such determination shall not affect the validity and enforceability of any other remaining provisions.</paragraph><paragraph>In addition to other prohibitions as set forth in the Terms of Service, you are prohibited from using our service: (a) for any unlawful purpose; (b) to solicit others to perform or participate in any unlawful acts; (c) to violate any international, federal, provincial or state regulations, rules, laws, or local ordinances; (d) to infringe upon or violate our intellectual property rights or the intellectual property rights of others; (e) to harass, abuse, insult, harm, defame, slander, disparage, intimidate, or discriminate based on gender, sexual orientation, religion, ethnicity, race, age, national origin, or disability; (f) to submit false or misleading information; (g) to upload or transmit viruses or any other type of malicious code that will or may be used in any way that will affect the functionality or operation of the Service or of any related website, other websites, or the Internet; (h) to collect or track the personal information of others; (i) to spam, phish, pharm or pretext; (j) for any obscene or immoral purpose; or (k) to interfere with or circumvent the security features of the Service or any related website, other websites, or the Internet. We reserve the right to terminate your use of the Service or any related website for violating any of the prohibited uses.</paragraph><paragraph>The failure of us to exercise or enforce any right or provision of these Terms of Service shall not constitute a waiver of such right or provision.</paragraph><paragraph>These Terms of Service and any policies or operating rules posted by us on this site or in respect to The Service constitutes the entire agreement and understanding between you and us and govern your use of the Service, superseding any prior or contemporaneous agreements, communications and proposals, whether oral or written, between you and us (including, but not limited to, any prior versions of the Terms of Service).</paragraph><paragraph>Any ambiguities in the interpretation of these Terms of Service shall not be construed against the drafting party.</paragraph><paragraph>Our order process is conducted by our online reseller Paddle.comarrow-up-right. Paddle.com is the Merchant of Record and authorized reseller for all the Services provided by us which means that you acquire our Services from Paddle.com.</paragraph><paragraph>By agreeing to these Terms of Service you also agree to Paddle's Buyer Terms and Conditions (https://paddle.com/legal-buyers/arrow-up-right).</paragraph><paragraph>Paddle.com handles returns and provides all customer service inquiries.</paragraph><paragraph>Licence Agreement</paragraph><paragraph>By purchasing the Services provided by Tardis.dev ("the Supplier"), you (the "Customer") are agreeing to this Licence Agreement ("Agreement"). This Agreement (as well as the documents referred to in it, including the Supplier's Privacy Policy and any additional terms or policies that the Supplier tells the Customer about) sets out the agreement between the Customer and the Supplier - please read them carefully.</paragraph><paragraph>1. Interpretation</paragraph><paragraph>1.1 The definitions and rules of interpretation in this clause apply in this Agreement and in any other agreement between the parties.</paragraph><paragraph>Authorised Person: means in relation to either party: (i) any director, officer, employee or professional advisor of that party to whom the disclosure of Confidential Information is necessary in order to enable that party to perform obligations or exercise rights pursuant to this Agreement; (ii) any body which regulates that party in any jurisdiction, if disclosure to that body is mandated by applicable law or relevant regulation; (iii) the insurers, brokers and auditors of that party; and (v) any service providers providing administrative and similar support services to that party in the ordinary course of business in connection with the performance of obligations under this Agreement and to whom disclosure of Confidential Information is necessary to enable that party to perform obligations or exercise rights pursuant to this Agreement.</paragraph><paragraph>Confidential Information: all financial, business and technical and all other information (regardless of its form or the medium in which it is stored) concerning the business and affairs of a party or of a confidential nature that the other party obtains, receives or has access to, before or after the date of this Agreement, in connection with, or in the performance of, the Agreement.</paragraph><paragraph>Customer System: any information technology system or systems owned or operated by the Customer to which Data is delivered or within which Data is Distributed in accordance with this Agreement.</paragraph><paragraph>Customer User: any employee of the Customer authorized by the Customer to access and use the Services (wholly or in part), including employees of Customer's Affiliates.</paragraph><paragraph>Customer Affiliate: an entity that owns or controls, is owned or controlled by or is or under common control or ownership with Customer, where control is defined as the possession, directly or indirectly, of the power to direct or cause the direction of the management and policies of an entity, whether through ownership of voting securities, by contract or otherwise;</paragraph><paragraph>Customer User Restrictions: the obligations set out in Schedule 1.</paragraph><paragraph>Data: the data or information, in whatever form including images, still and moving, and including financial and market research information, the provision of which comprises the Services (wholly or in part).</paragraph><paragraph>Derived Data: any Data (wholly or in part):</paragraph><list type="ol"><item>Manipulated to such a degree that it: (i) cannot be identified as originating or deriving directly from the Data or the Services and cannot be reverse-engineered such that it can be so identified; (ii) is not capable of use substantially as a substitute for the Data or the Services;</item><item>that is not separately marketed by the Customer; and</item><item>that has no independent commercial value.</item></list><paragraph>Manipulated to such a degree that it: (i) cannot be identified as originating or deriving directly from the Data or the Services and cannot be reverse-engineered such that it can be so identified; (ii) is not capable of use substantially as a substitute for the Data or the Services;</paragraph><paragraph>that is not separately marketed by the Customer; and</paragraph><paragraph>that has no independent commercial value.</paragraph><paragraph>Distribute: to make Data accessible (including the provision of access through a database or other application populated with the Data, transferring or disclosing the Data) by any means, including any electronic means, to any Customer User.</paragraph><paragraph>Effective Date: date of acceptance of the Agreement by the Customer.</paragraph><paragraph>Fees: fees specified on the Website and in the purchase invoice.</paragraph><paragraph>Force Majeure Event: means an event beyond a Party's reasonable control (but in each case only to the extent actually beyond the control of the Party seeking to rely on that event as a Force Majeure Event), including: (i) extreme abnormal weather conditions; (ii) nuclear, chemical or biological contamination; (iii) war, civil commotion or terrorist attack; (iv) interruption or failure of a utility service including electric power, gas or water; (v) acts of God, floods or earthquakes; (vi) pandemic (excluding COVID-19); or (vii) the imposition of a sanction, embargo or breaking off of diplomatic relations, but excluding in each case strikes or other forms of industrial action by the employees, agents or subcontractors of that Party, or any change in applicable law or relevant regulation.</paragraph><paragraph>Initial Period: the period commencing on the Effective Date that is specified in the purchase invoice.</paragraph><paragraph>Insolvency Event: (i) any procedure commenced with a view to the winding-up or re-organisation of such party; (ii) any step taken or any procedure is commenced with a view to the appointment of an administrator, receiver, administrative receiver or trustee in bankruptcy in relation to such party or all or substantially all of its assets; (iii) the holder of any security over all or substantially all of the assets of such party takes any step to enforce that security; (iv) all or substantially all of the assets of such party is subject to attachment, sequestration, execution or any similar process; (v) such party is unable to pay its debts as they fall due; (vi) such party enters into, or any step is taken, whether by the board of directors of such party or otherwise, towards entering into a composition or arrangement with its creditors or any class of them, including a company voluntary arrangement or a deed of arrangement; or (vii) such party enters into, or any step is taken, whether by the board of directors of such party or otherwise, towards any analogous procedure under the laws of any jurisdiction to the procedures set out in (i) to (vi) above.</paragraph><paragraph>Intellectual Property Rights: means: (i) rights in, and in relation to, any patents, registered designs, design rights, trade marks, trade and business names (including goodwill associated with any trade marks or trade and business names), copyright and related rights, moral rights, databases, domain names, semi-conductor and other topography rights and utility models, and including registrations and applications for, and renewals or extensions of, such rights, and similar or equivalent rights or forms of protection in any part of the world; (ii) rights in the nature of unfair competition rights and to sue for passing off and for past infringement; and (iii) trade secrets, confidentiality and other proprietary rights, including rights to know how and other technical information.</paragraph><paragraph>Licence: the licence granted in Clause 9.</paragraph><paragraph>Manipulate: to combine or aggregate the Data (wholly or in part) with other data or information or to adapt the Data (wholly or in part).</paragraph><paragraph>Manipulated Data: any Data which has been Manipulated. Manipulated Data includes any Derived Data.</paragraph><paragraph>**Mark:**means the trade marks, trade names, product or service names, logos, slogans, typefaces, brand or other proprietary words or symbols used by the Supplier from time to time.</paragraph><paragraph>Materials: any documents or software supplied by the Supplier under this Agreement.</paragraph><paragraph>Permitted Use: internal business use (which shall not include the use of the Data or the Materials by, or for the benefit of, any person other than an employee of the Customer).</paragraph><paragraph>Release: generally available upgrades and enhancements to the Data.</paragraph><paragraph>Services: the services to be supplied by the Supplier under this Agreement, including the supply of any Data, Materials, or Support.</paragraph><paragraph>Software: any software provided by the Supplier to enable the Services to be used including any Releases.</paragraph><paragraph>Support: the support to be supplied by the Supplier including reasonable efforts to assist the Customer to access the Data.</paragraph><paragraph>Term: the Initial Period and any Renewal Periods.</paragraph><paragraph>Website: means any webpage of the Supplier, including but not limited to Tardis.devarrow-up-right.</paragraph><paragraph>1.2 The headings in this Agreement are inserted for convenience only and shall not affect its construction.</paragraph><paragraph>1.3 A person includes a natural person, corporate or unincorporated body (whether or not having separate legal personality).</paragraph><paragraph>1.4 The schedules form part of this Agreement and shall have effect as if set out in full in the body of this Agreement. Any reference to this Agreement includes the schedules.</paragraph><paragraph>1.5 A reference to a company shall include any company, corporation or other body corporate, wherever and however incorporated or established.</paragraph><paragraph>1.6 Unless the context otherwise requires, words in the singular shall include the plural and in the plural shall include the singular.</paragraph><paragraph>1.7 A reference to a particular law is a reference to it as it is in force for the time being taking account of any amendment, extension, or re-enactment and includes any subordinate legislation for the time being in force made under it.</paragraph><paragraph>1.8 References to clauses and schedules are to the clauses and schedules of this Agreement and references to paragraphs are to paragraphs of the relevant schedule.</paragraph><paragraph>1.9 Any words following the terms including, include, in particular or for example or any similar phrase shall be construed as illustrative and shall not limit the generality of the related general words.</paragraph><paragraph>1.10 If there is any uncertainty between any provision contained in the body of this Agreement and any provision contained in the Schedules or appendices, the provision in the body of this Agreement shall prevail.</paragraph><paragraph>During the Term the Supplier shall supply the Services to the Customer and the Customer shall pay the Fees and use the Services.</paragraph><paragraph>3.1 The Supplier shall use reasonable efforts to make connection to the Services available on the Effective Date.</paragraph><paragraph>3.2 The Customer shall ensure that it promptly complies with any minimum hardware configuration requirements specified by the Supplier to establish connectivity between the Customer System and the Services.</paragraph><paragraph>3.3 Each party shall bear its own costs of establishing that connectivity.</paragraph><paragraph>4.1 During the Term the Supplier shall supply the Services to the Customer.</paragraph><paragraph>4.2 The Supplier may change at any time, with as much prior notice to the Customer as is reasonably practicable:</paragraph><list type="ol"><item>the content, format or nature of the Data or the Services; and</item><item>the means of access to the Data or the Services.</item></list><paragraph>the content, format or nature of the Data or the Services; and</paragraph><paragraph>the means of access to the Data or the Services.</paragraph><paragraph>5.1 Customer will pay to Supplier, without offset or deduction, all fees due under this Agreement. Unless otherwise specified, all fees shall be due 14 days from the date of invoice and all fees are non-cancelable and non-refundable.</paragraph><paragraph>5.2 Supplier order process is conducted by Supplier online reseller Paddle.com. Paddle.com is the Merchant of Record for all Supplier orders.</paragraph><paragraph>5.3 All Fees are exclusive of VAT or any other applicable sales tax, which shall be paid by the Customer at the rate and in the manner for the time being prescribed by law.</paragraph><paragraph>6. Confidentiality</paragraph><paragraph>6.1 The parties shall each, as a receiving party: (i) keep confidential all Confidential Information disclosed by the disclosing party; (ii) shall not use the Confidential Information disclosed by the disclosing party in any way contrary to this Agreement, including Schedule I and not otherwise for the benefit of any third party; and (iii) not disclose the Confidential Information disclosed by the disclosing party to any person save to an Authorised Person.</paragraph><paragraph>6.2 The parties shall each, as a receiving party, ensure that each Authorised Person complies with confidentiality provisions no less onerous than those contained in this Clause 6 and will remain liable for any disclosure of Confidential Information by each Authorised Person as if it had made such disclosure.</paragraph><paragraph>6.3 The parties shall each, on the other party's request destroy, erase or deliver to the other party all the requesting party's Confidential Information, save where the retention of such Confidential Information is necessary to comply with applicable law or relevant regulation or otherwise for the other party to exercise its rights or receive benefits due under the Agreement.</paragraph><paragraph>6.4 The parties agree that the provisions of Clauses 6.1, 6.2, and 6.3 shall not apply to any information which the receiving party can prove: (i) is or becomes public knowledge other than by breach of this Clause; (ii) was in the possession of the receiving party without restriction in relation to disclosure before the date of receipt from the disclosing party; (iii) is received from a third party who lawfully acquired it and who was under no obligation restricting its disclosure; or (iv) was independently developed without access to any Confidential Information disclosed by the disclosing party.</paragraph><paragraph>6.5 The parties agree that these provisions in this Clause 6 shall not apply so as to prevent disclosure of Confidential Information by the receiving party to the extent that such disclosure is required to be made by any authority of competent jurisdiction or by any applicable law or relevant regulation or for the purposes of defending itself in relation to actual or threatened proceedings, regardless of whether brought or threatened by the other party or any other person, provided in each case that where permissible the receiving party: (i) gives the disclosing party reasonable formal written notice (provided that this is not in contravention of applicable law or relevant regulation) prior to such disclosure to allow the disclosing party a reasonable opportunity to seek a protective order; and (ii) uses reasonable endeavours to obtain prior to the disclosures written assurance from the applicable entity that it will keep the Confidential Information confidential.</paragraph><paragraph>6.6 Each party reserves all rights in its Confidential Information. No rights or obligations in respect of a party's Confidential Information, other than those expressly stated in this Agreement, are granted to the other party, or are to be implied from this Agreement.</paragraph><paragraph>6.7 The provisions of this Clause 6 shall survive any expiry or termination of the Agreement.</paragraph><paragraph>7. Announcements</paragraph><paragraph>No party shall make, or permit any person to make, any public announcement concerning this Agreement without the prior written consent of the other party (such consent not to be unreasonably withheld or delayed), except as required by law, any governmental or regulatory authority (including any relevant securities exchange), any court or other authority of competent jurisdiction</paragraph><paragraph>The Customer shall ensure that the Data and Materials are kept secure, and shall use security practices and systems consistent with standard industry practices and which will be applicable to the use of the Data and Materials to prevent, and take prompt and proper remedial action against, unauthorised access, copying, modification, storage, reproduction, display or distribution of the Data and the Materials.</paragraph><paragraph>9.1 The Supplier grants to the Customer a non-exclusive, non-transferable, licence for the Permitted Use only, subject to the Customer User Restrictions, to:</paragraph><list type="ol"><item>access, view and Manipulate Data and create Derived Data;</item><item>store the Data and Manipulated Data on the Customer System;</item><item>Distribute Derived Data to Customer Users on the Customer System; and</item><item>use (but not modify) the Materials in support of the activities referred to in this Clause 9.1.</item></list><paragraph>access, view and Manipulate Data and create Derived Data;</paragraph><paragraph>store the Data and Manipulated Data on the Customer System;</paragraph><paragraph>Distribute Derived Data to Customer Users on the Customer System; and</paragraph><paragraph>use (but not modify) the Materials in support of the activities referred to in this Clause 9.1.</paragraph><paragraph>9.2 Except as expressly provided in this Agreement, the Customer shall not:</paragraph><list type="ol"><item>use the Services (wholly or in part) in its products or services; or</item><item>redistribute or resell the Data or the Services (wholly or in part) with exception of reselling or redistributing aggregated and calculated data (lowest resolution being 10 minutes)</item></list><paragraph>use the Services (wholly or in part) in its products or services; or</paragraph><paragraph>redistribute or resell the Data or the Services (wholly or in part) with exception of reselling or redistributing aggregated and calculated data (lowest resolution being 10 minutes)</paragraph><paragraph>9.3 The Customer shall comply with the Customer User Restrictions.</paragraph><paragraph>10. Intellectual property rights ownership</paragraph><paragraph>10.1 The Customer acknowledges that:</paragraph><list type="ol"><item>all Intellectual Property Rights in the Data and the Materials are the property of the Supplier or its licensors, as the case may be;</item><item>it shall have no rights in or to the Data or the Materials other than the right to use them in accordance with the express terms of this Agreement;</item><item>the Supplier or its licensors has or have made and will continue to make substantial investment in the obtaining, verification, selection, coordination, development, presentation and supply of the Data;</item><item>it shall use the Supplier's Mark strictly in accordance with the Supplier's written instructions; and</item><item>any goodwill generated though the Customer's use of the Supplier's Mark shall belong only to the Supplier.</item></list><paragraph>all Intellectual Property Rights in the Data and the Materials are the property of the Supplier or its licensors, as the case may be;</paragraph><paragraph>it shall have no rights in or to the Data or the Materials other than the right to use them in accordance with the express terms of this Agreement;</paragraph><paragraph>the Supplier or its licensors has or have made and will continue to make substantial investment in the obtaining, verification, selection, coordination, development, presentation and supply of the Data;</paragraph><paragraph>it shall use the Supplier's Mark strictly in accordance with the Supplier's written instructions; and</paragraph><paragraph>any goodwill generated though the Customer's use of the Supplier's Mark shall belong only to the Supplier.</paragraph><paragraph>10.2 The Customer acknowledges that reference in any element of the Materials to trade names or proprietary products where no specific acknowledgement of such names or products is made does not imply that such names or products may be regarded by the Customer as free for general use, outside the scope of the use of the Materials authorised by this agreement.</paragraph><paragraph>10.3 If any third-party claim is made, or in the Supplier's reasonable opinion is likely to be made, in relation to the use of the Data, the Supplier may at its sole option and expense:</paragraph><list type="ol"><item>procure for the Customer the right to continue using, developing, modifying or retaining the Data or the Materials (wholly or in part) in accordance with this Agreement;</item><item>modify the Data or the Materials (wholly or in part) so that they cease to be infringing;</item><item>replace the Data or the Materials (wholly or in part) with non-infringing items; or</item><item>terminate this Agreement immediately by notice in writing to the Customer. In respect of ongoing Subscriptions purchased by the Customer, the Supplier shall refund any Fees for the Initial Period or Renewal Period (as relevant) paid in advance by the Customer as at the date of termination (less a reasonable sum in respect of the Customer's use of the Data or Materials to the date of termination) on return of the Data or the Materials and all copies of each of them.</item></list><paragraph>procure for the Customer the right to continue using, developing, modifying or retaining the Data or the Materials (wholly or in part) in accordance with this Agreement;</paragraph><paragraph>modify the Data or the Materials (wholly or in part) so that they cease to be infringing;</paragraph><paragraph>replace the Data or the Materials (wholly or in part) with non-infringing items; or</paragraph><paragraph>terminate this Agreement immediately by notice in writing to the Customer. In respect of ongoing Subscriptions purchased by the Customer, the Supplier shall refund any Fees for the Initial Period or Renewal Period (as relevant) paid in advance by the Customer as at the date of termination (less a reasonable sum in respect of the Customer's use of the Data or Materials to the date of termination) on return of the Data or the Materials and all copies of each of them.</paragraph><paragraph>10.4 Supplier shall indemnify and hold Customer harmless for any claims arising from third-party claims of any infringement of Intellectual Property Rights.</paragraph><paragraph>11.1 Except as expressly stated in this Agreement, all warranties, conditions and terms, whether express or implied by statute, common law or otherwise (including any implied warranties of satisfactory quality or fitness for a particular purpose or non-infringement) are hereby excluded to the extent permitted by law.</paragraph><paragraph>11.2 Without limiting the effect of Clause 11.1, the Supplier does not warrant or make any representations:</paragraph><list type="ol"><item>that the supply of the Data will be error-free, free from interruption,or operate without loss or corruption of data or technical malfunction;</item><item>that the Data is accurate, complete, reliable, secure, useful, fit for purpose or timely; or</item><item>that the Data has been tested for use by the Customer or any third party or that the Data will be suitable for or be capable of being used by the Customer or any third party; or</item><item>regarding the benefit the Customer or any third party will obtain from the Data.</item></list><paragraph>that the supply of the Data will be error-free, free from interruption,or operate without loss or corruption of data or technical malfunction;</paragraph><paragraph>that the Data is accurate, complete, reliable, secure, useful, fit for purpose or timely; or</paragraph><paragraph>that the Data has been tested for use by the Customer or any third party or that the Data will be suitable for or be capable of being used by the Customer or any third party; or</paragraph><paragraph>regarding the benefit the Customer or any third party will obtain from the Data.</paragraph><paragraph>12. Limitation of liability</paragraph><paragraph>12.1 The Customer acknowledges that:</paragraph><list type="ol"><item>the use and interpretation of the Data requires specialist skill and knowledge of financial markets;</item><item>the Customer has that skill and knowledge and undertakes that it will exercise that skill and knowledge and appropriate judgment when using the Data;</item><item>the Customer shall be solely responsible, as against the Supplier, for any opinions, recommendations, forecasts or other conclusions made or actions taken by the Customer, any client of the Customer or any other third party based (wholly or in part) on the Data unless otherwise set out in this Clause 12; and</item><item>it is in the best position to ascertain any likely loss it may suffer in connection with this Agreement, that it is therefore responsible for making appropriate insurance arrangements to address the risk of any such loss and that the provisions of this Clause 12 are reasonable in these circumstances.</item></list><paragraph>the use and interpretation of the Data requires specialist skill and knowledge of financial markets;</paragraph><paragraph>the Customer has that skill and knowledge and undertakes that it will exercise that skill and knowledge and appropriate judgment when using the Data;</paragraph><paragraph>the Customer shall be solely responsible, as against the Supplier, for any opinions, recommendations, forecasts or other conclusions made or actions taken by the Customer, any client of the Customer or any other third party based (wholly or in part) on the Data unless otherwise set out in this Clause 12; and</paragraph><paragraph>it is in the best position to ascertain any likely loss it may suffer in connection with this Agreement, that it is therefore responsible for making appropriate insurance arrangements to address the risk of any such loss and that the provisions of this Clause 12 are reasonable in these circumstances.</paragraph><paragraph>12.2 Neither party excludes or limits liability to the other party for:</paragraph><list type="ol"><item>fraud or fraudulent misrepresentation;</item><item>any matter which cannot be excluded by law.</item></list><paragraph>fraud or fraudulent misrepresentation;</paragraph><paragraph>any matter which cannot be excluded by law.</paragraph><paragraph>12.3 Subject to Clause 12.2, each party shall not in any circumstances be liable whether in contract, tort (including for negligence and breach of statutory duty howsoever arising), misrepresentation (whether innocent or negligent), restitution or otherwise, for:</paragraph><list type="ol"><item>any loss (whether direct or indirect) of profits, business, business opportunities, revenue, turnover, reputation or goodwill;</item><item>any loss or corruption (whether direct or indirect) of data or information;</item><item>loss (whether direct or indirect) of anticipated savings or wasted expenditure (including management time); or</item><item>any loss or liability (whether direct or indirect) under or in relation to any other contract.</item></list><paragraph>any loss (whether direct or indirect) of profits, business, business opportunities, revenue, turnover, reputation or goodwill;</paragraph><paragraph>any loss or corruption (whether direct or indirect) of data or information;</paragraph><paragraph>loss (whether direct or indirect) of anticipated savings or wasted expenditure (including management time); or</paragraph><paragraph>any loss or liability (whether direct or indirect) under or in relation to any other contract.</paragraph><paragraph>12.4 Subject to Clause 12.2 and excluding Clause 10.4, each party's total aggregate liability in contract, tort (including negligence and breach of statutory duty howsoever arising), misrepresentation (whether innocent or negligent), restitution or otherwise, arising in connection with the performance or contemplated performance of this Agreement or any collateral contract shall in all circumstances be limited to the total Fees paid by the Customer to the Supplier during the 12-month period immediately before the date on which the cause of action first arose or, if the cause of actions arose during the Initial Period, in respect of the Initial Period.</paragraph><paragraph>12.5 The Supplier shall not be liable for any delay in delivery of the Services that is caused by an event within the scope of Clause 14 or the Customer's failure to provide the Supplier with adequate delivery instructions or any other instructions that are relevant to the supply of the Services or the Customer's failure to comply with Clause 3.2.</paragraph><paragraph>13. Term and termination</paragraph><paragraph>13.1 This Agreement shall commence on the Effective Date. Unless terminated earlier in accordance with this Clause 13 or Clause 10.3, this Agreement shall continue for the Initial Period.</paragraph><paragraph>13.2 The Supplier may terminate this Agreement in respect of the Services (wholly or in part):</paragraph><list type="ol"><item>with immediate effect by giving written notice to the Customer if the Customer fails to pay any amount due under this Agreement on the due date for payment and remains in default not less than 30 days after being notified in writing to make that payment;</item><item>on written notice to the Customer at any time if the Supplier discontinues or withdraws, in whole or in part, its provision of the Services in question to all subscribers of such Services. The Supplier will use reasonable endeavours to give the Customer as much notice of the same as reasonably practicable, but any such termination will be without liability to the Supplier.</item></list><paragraph>with immediate effect by giving written notice to the Customer if the Customer fails to pay any amount due under this Agreement on the due date for payment and remains in default not less than 30 days after being notified in writing to make that payment;</paragraph><paragraph>on written notice to the Customer at any time if the Supplier discontinues or withdraws, in whole or in part, its provision of the Services in question to all subscribers of such Services. The Supplier will use reasonable endeavours to give the Customer as much notice of the same as reasonably practicable, but any such termination will be without liability to the Supplier.</paragraph><paragraph>13.3 Without prejudice to any rights that have accrued under this Agreement or any of its rights or remedies, either party may terminate this Agreement (or any part thereof) with immediate effect by giving written notice to the other party if:</paragraph><list type="ol"><item>the other party: (i) commits a material breach of this Agreement and (if that breach is remediable) fails to remedy that breach within a period of 30 days after being notified in writing to do so; or (ii) commits a series of breaches of this Agreement which when taken together have the impact or effect of or otherwise amount to a material breach;</item><item>a Force Majeure Event continues for a period exceeding two (2) months;</item><item>the other party becomes subject to an Insolvency Event; or</item><item>the party reasonably determines that it has become unlawful to perform its obligations under the Agreement.</item></list><paragraph>the other party: (i) commits a material breach of this Agreement and (if that breach is remediable) fails to remedy that breach within a period of 30 days after being notified in writing to do so; or (ii) commits a series of breaches of this Agreement which when taken together have the impact or effect of or otherwise amount to a material breach;</paragraph><paragraph>a Force Majeure Event continues for a period exceeding two (2) months;</paragraph><paragraph>the other party becomes subject to an Insolvency Event; or</paragraph><paragraph>the party reasonably determines that it has become unlawful to perform its obligations under the Agreement.</paragraph><paragraph>13.4 Any provision of this Agreement that expressly or by implication is intended to come into or continue in force on or after termination of this Agreement shall remain in full force and effect.</paragraph><paragraph>13.5 Termination or expiry of this agreement shall not affect any rights, remedies, obligations or liabilities of the parties that have accrued up to the date of termination or expiry, including the right to claim damages in respect of any breach of the agreement which existed at or before the date of termination or expiry.</paragraph><paragraph>13.6 On any termination of this Agreement for any reason or expiry of the Term, the Customer shall:</paragraph><list type="ol"><item>immediately pay any outstanding amounts owed to the Supplier under this Agreement; and</item><item>within a reasonable period of termination or expiry ensure that there is no further use of the Services in any of the Customer's products, applications or services, provided that the Customer shall not be obliged to remove from its products, applications and services any Data or Derived Data incorporated into them in accordance with this Agreement before termination or expiry.</item></list><paragraph>immediately pay any outstanding amounts owed to the Supplier under this Agreement; and</paragraph><paragraph>within a reasonable period of termination or expiry ensure that there is no further use of the Services in any of the Customer's products, applications or services, provided that the Customer shall not be obliged to remove from its products, applications and services any Data or Derived Data incorporated into them in accordance with this Agreement before termination or expiry.</paragraph><paragraph>13.7 On termination of this Agreement for any reason (save for termination for material breach by the Customer under Clause 13.3(a) or for failure to pay amounts due under Clause 13.2(a)), the Supplier shall refund any Fees for the Initial Period or Renewal Period (as relevant) paid in advance by the Customer as at the date of termination or expiry (less a reasonable sum taking into account the remaining length of the Initial Period or Renewal Period and the Customer's use of the Data or the Materials to the date of termination). If the Supplier terminates this Agreement under Clause 13.3(a) due to the Customer's material breach, or under Clause 13.2(a) for the Customer's failure to pay amounts due, the Customer shall not be entitled to any refund.</paragraph><paragraph>14. Force Majeure</paragraph><paragraph>Neither party shall be responsible for any failure to fulfill any obligation for so long as, and to the extent to which, the fulfillment of such obligation is impeded by a Force Majeure Event, and the affected party:</paragraph><list type="ol"><item>has promptly notified the other party of any circumstances which may result in failure to perform its obligations;</item><item>uses its best endeavours to minimize the adverse consequences that any failure in performance of its obligations might have, and to return the performance of such obligations to normal as soon as possible.</item></list><paragraph>has promptly notified the other party of any circumstances which may result in failure to perform its obligations;</paragraph><paragraph>uses its best endeavours to minimize the adverse consequences that any failure in performance of its obligations might have, and to return the performance of such obligations to normal as soon as possible.</paragraph><paragraph>15.1 This Agreement is personal to the Customer and it shall not assign, transfer, mortgage, charge, sub-contract, or otherwise transfer any of its rights and obligations under this Agreement without the prior written consent of the Supplier (which is not to be unreasonably withheld or delayed). Notwithstanding the preceding, Customer may assign any right or obligations to any of its Affiliates. For the purposes of this Agreement, the term "Affiliate" shall mean in respect of a party, any other entity that directly or indirectly controls, is controlled by or is under common control with, that party.</paragraph><paragraph>15.2 The Supplier may at any time assign, transfer, mortgage, charge, sub-contract, or otherwise transfer any of its rights and obligations under this Agreement without the consent of the Customer.</paragraph><paragraph>No failure or delay by a party to exercise any right or remedy provided under the Agreement or by law, or a single or partial exercise of such right or remedy, shall constitute a waiver of that or any other right or remedy, nor shall it preclude or restrict the further exercise of that or any other right or remedy.</paragraph><paragraph>Except as expressly provided in this Agreement, the rights and remedies provided under this Agreement are in addition to, and not exclusive of, any rights or remedies provided by law.</paragraph><paragraph>All notices, demands and other communications provided for or permitted under this Agreement will be made in writing to the parties at the addresses on the Cover Page and will be sent by email and will be deemed received upon receipt of a delivery receipt.</paragraph><paragraph>19. Entire Agreement</paragraph><paragraph>19.1 This Agreement represents the entire agreement between the parties and supersedes all previous discussions, correspondence, negotiations, arrangements, understandings and agreements between them, whether written or oral, relating to its subject matter.</paragraph><paragraph>19.2 Each party acknowledges that in entering into this Agreement it does not rely on, and shall have no remedies in respect of, any representation or warranty (whether made innocently or negligently) that is not set out in this Agreement.</paragraph><paragraph>19.3 Each party agrees that it shall have no claim for innocent or negligent misrepresentation based on any statement in this agreement.</paragraph><paragraph>The Supplier reserves the right to change the Agreement at any time and the Customer should revisit the terms and conditions at Tardis.devarrow-up-right before making purchase to ensure that it is fully aware of the current terms and conditions.</paragraph><paragraph>21. No partnership or agency</paragraph><paragraph>Nothing in this agreement is intended to, or shall be deemed to, establish any partnership or joint venture between any of the parties, constitute any party the agent of another party, or authorise any party to make or enter into any commitments for or on behalf of any other party.</paragraph><paragraph>22. Third-party rights</paragraph><paragraph>Except as expressly provided in this Agreement, a person who is not a party to this Agreement shall not have any rights under the Contracts (Rights of Third Parties) Act 1999 or otherwise to enforce any term of this Agreement.</paragraph><paragraph>23. Coinbase market data license restrictions</paragraph><paragraph>In relation to licensing Coinbase market data additional restrictions apply.</paragraph><paragraph>23.1 Customer is prohibited from further disseminating Coinbase Data and/or Derivative Data, and using or permitting the use of Coinbase Data, Derivative Data and/or any part thereof for any Prohibited Use that means any of:</paragraph><list type="ol"><item>use of Coinbase Data or Derivative Data that violates any applicable laws or the terms and conditions of this Agreement,</item><item>use of Coinbase Data to create Financial Products,</item><item>display or redistribution of any Coinbase Data or Derivative Datato any third party who is not a Customer,or by any Customer to any third party,</item><item>authorization of any Person to do any of the foregoing,</item><item>any other use not expressly permitted under this Agreement, and</item><item>any other use of the Data not expressly permitted under the Coinbase Market Data Policy.</item></list><paragraph>use of Coinbase Data or Derivative Data that violates any applicable laws or the terms and conditions of this Agreement,</paragraph><paragraph>use of Coinbase Data to create Financial Products,</paragraph><paragraph>display or redistribution of any Coinbase Data or Derivative Data</paragraph><list type="ol"><item>to any third party who is not a Customer,</item><item>or by any Customer to any third party,</item></list><paragraph>to any third party who is not a Customer,</paragraph><paragraph>or by any Customer to any third party,</paragraph><paragraph>authorization of any Person to do any of the foregoing,</paragraph><paragraph>any other use not expressly permitted under this Agreement, and</paragraph><paragraph>any other use of the Data not expressly permitted under the Coinbase Market Data Policy.</paragraph><paragraph>23.2 Customer agrees that Coinbase may inspect Customer's use of Coinbase Data and/or Derivative Data by Licensee and its agents upon ten (10) days advance notice to Customer; and Supplier or Coinbase has the ability to modify the agreement (or any other agreements related to the use of Coinbase Data or Derivative Data) with any Customer as Coinbase may from time to time specify, except that Supplier may continue to provide Coinbase Data and/or Derivative Data to a Customer without affecting the modification for ninety (90) days from that receipt; Supplier and Coinbase shall discontinue its provision of Coinbase Data and/or Derivative Data to a Customer thereafter if the Customer has not agreed to the modifications after this ninety (90) day period.</paragraph><paragraph>23.3 Customer agrees that for reporting purposes some of the customer specific information (such as customer name or address for example) may be be required by and reported to Coinbase.</paragraph><paragraph>SCHEDULE 1 - CUSTOMER USER RESTRICTIONS</paragraph><paragraph>(a) limit access to the Services to the Customer Users, which shall include Customer's Affiliates;</paragraph><paragraph>(b) only make copies of the Data and the Materials to the extent reasonably necessary for the following purposes: back-up, mirroring (and similar availability enhancement techniques), security, disaster recovery and testing;</paragraph><paragraph>(c) comply with all applicable law and relevant regulations, and not use the Services for any purpose contrary to any applicable law or relevant regulation, or any regulatory code, guidance or request;</paragraph><paragraph>(d) not extract, reutilise, use, exploit, redistribute, resell, redisseminate, copy or store the Data or the Materials for any purpose not expressly permitted by this Agreement;</paragraph><paragraph>(e) not copy, modify, decompile, reverse engineer or create derivative works from the Software, except to the extent permitted by any applicable law; and</paragraph><paragraph>(f) not do anything which may damage the reputation of the Supplier, the Data or the Services, including by way of using the Data (wholly or in part) in any manner which is pornographic, racist or that incites religious hatred or violence.</paragraph></content>
</page>
<page url="https://docs.tardis.dev/faq/data">
<title>Data | Tardis.dev Documentation</title>
<content><paragraph>What data types do you support?</paragraph><paragraph>We provide the most comprehensive and granular market data on the market sourced from real-time WebSocket APIs with complete control and transparency how the data is being recorded.</paragraph><paragraph>Via downloadable CSV data files following normalized tick-level data types are available:</paragraph><paragraph>Raw data API that is available for pro and business subscriptions provides data in exchange-native data format. See historical data details to learn about real-time channels captured for each exchange. Each captured channel can be considered a different exchange specific data type (for example Binance bookTicker channel, or BitMEX liquidation channel).</paragraph><paragraph>We also provide following normalized data types via our client libs (normalization is done client-side, using raw data API as a data source):</paragraph><list type="ul"><item>order book snapshots (tick-by-tick, 10ms, 100ms, 1s, 10s etc)</item><item>derivative tick info (open interest, funding rate, mark price, index price)</item><item>volume/tick based trade bars</item></list><paragraph>order book snapshots (tick-by-tick, 10ms, 100ms, 1s, 10s etc)</paragraph><paragraph>derivative tick info (open interest, funding rate, mark price, index price)</paragraph><paragraph>volume/tick based trade bars</paragraph><paragraph>What does high frequency historical data mean?</paragraph><paragraph>We always collect and provide data with the most granularity that exchange can offer via it's real-time WS feeds. High frequency can mean different things for different exchanges due to exchanges APIs limitations. For example for Coinbase Pro it can mean L3 order book data (market-by-order), for Binance Futures all order book L2 real-time updates and for Binance Spot it means order book updates aggregated in 100ms intervals.</paragraph><paragraph>How historical raw market data is being sourced?</paragraph><paragraph>Raw market data is sourced from exchanges real-time WebSocket APIs. For cases where exchange lacks WebSocket API for particular data type we fallback to pooling REST API periodically, e.g., Binance Futures open interest data.</paragraph><paragraph>Why data source matters and why we use real-time WebSocket feeds as data source vs periodically calling REST endpoints?</paragraph><paragraph>Recording exchanges real-time WebSocket feeds allows us preserving and providing the most granular data that exchanges APIs can offer including data that is simply not available via their REST APIs like tick level order book updates.</paragraph><paragraph>Historical data sourced from WebSocket real-time feeds adheres to what you'll see when trading live and can be used to exactly replicate live conditions even if it means some occasional connection drops causing small data gaps, real-time data publishing delays especially during larger market moves, duplicated trades or crossed books in some edge cases. We find that trade-off acceptable and even if data isn't as clean and corrected as sourced from REST APIs, it allows for more insight into market microstructure and various unusual exchanges behaviors that simply can't be captured otherwise.</paragraph><paragraph>Simple example would be latency spikes for many exchanges during increased volatility periods where exchange publish trade/order book/quote WebSocket messages with larger than usual latency or simply skip some of the the updates and then return those in one batch. Querying the REST API would result in nice, clean trade history, but such data wouldn't fully reflect real actionable market behavior and would result in unrealistic backtesting results, breaking in the real-time scenarios.</paragraph><paragraph>What L2 order book data can be used for?</paragraph><paragraph>L2 data (market-by-price) includes bids and asks orders aggregated by price level and can be used to analyze among other things:</paragraph><list type="ul"><item>average liquidity away from midpoint</item><item>hidden interest (i.e., iceberg orders)</item></list><paragraph>average liquidity away from midpoint</paragraph><paragraph>hidden interest (i.e., iceberg orders)</paragraph><paragraph>We do provide L2 data both in CSV format as incremental order book L2 updates, tick level order book snapshots (top 25 and top 5 levels) as well as in exchange-native format via API and client libraries that can perform full order book reconstruction client-side.</paragraph><paragraph>What L3 order book data can be used for?</paragraph><paragraph>L3 data (market-by-order) includes every order book order addition, update, cancellation and match and can be used to analyze among other things:</paragraph><paragraph>Historical L3 data is currently available via API for Bitfinex, Coinbase Pro and Bitstamp - remaining supported exchanges provide L2 data only.</paragraph><paragraph>What is the maximum order book depth available for each supported exchange?</paragraph><paragraph>We always collect full depth order book data as long as exchange's WebSocket API supports it. Table below shows current state of affairs for each supported exchange.</paragraph><paragraph>order book updates frequency</paragraph><paragraph>full order book depth snapshot and updates</paragraph><paragraph>full order book depth snapshot and updates</paragraph><paragraph>top 1000 levels initial order book snapshot, full depth incremental order book updates</paragraph><paragraph>real-time, dynamically adjusted</paragraph><paragraph>top 1000 levels initial order book snapshot, full depth incremental order book updates</paragraph><paragraph>real-time, dynamically adjusted</paragraph><paragraph>top 1000 levels initial order book snapshot, full depth incremental order book updates</paragraph><paragraph>top 100 levels initial order book snapshot and updates</paragraph><paragraph>top 400 levels initial order book snapshot and updates</paragraph><paragraph>top 400 levels initial order book snapshot and updates</paragraph><paragraph>top 400 levels initial order book snapshot and updates</paragraph><paragraph>top 400 levels initial order book snapshot and updates</paragraph><paragraph>top 150 levels initial order book snapshot and updates</paragraph><paragraph>top 150 levels initial order book snapshot and updates</paragraph><paragraph>top 150 levels initial order book snapshot and updates</paragraph><paragraph>top 150 levels initial order book snapshot and updates</paragraph><paragraph>top 100 levels initial order book snapshot and updates</paragraph><paragraph>top 100 levels initial order book snapshot and updates</paragraph><paragraph>full order book depth snapshot and updates</paragraph><paragraph>full order book depth snapshot and updates</paragraph><paragraph>top 1000 levels initial order book snapshot and updates</paragraph><paragraph>full order book depth snapshot and updates</paragraph><paragraph>full order book depth snapshot and updates</paragraph><paragraph>full order book depth snapshot and updates</paragraph><paragraph>top 25 levels initial order book snapshot and updates</paragraph><paragraph>full order book depth snapshot and updates</paragraph><paragraph>top 30 levels initial order book snapshot and updates</paragraph><paragraph>top 100 levels initial order book snapshot and updates</paragraph><paragraph>top 1000 levels initial order book snapshot, full depth incremental order book updates</paragraph><paragraph>top 20 levels order book snapshots</paragraph><paragraph>top 30 levels order book snapshots</paragraph><paragraph>top 15 levels order book snapshots</paragraph><paragraph>top 15 levels order book snapshots</paragraph><paragraph>top 400 levels initial order book snapshot and updates</paragraph><paragraph>full order book depth snapshot and updates</paragraph><paragraph>full order book depth snapshot and updates</paragraph><paragraph>top 1000 levels initial order book snapshot, full depth incremental order book updates</paragraph><paragraph>Which exchanges support liquidations data type?</paragraph><paragraph>Liquidationsarrow-up-right data is sourced from exchanges WebSocket APIs when supported with fallback to pooling REST APIs when WebSockets APIs do not support that data type and can be accessed via raw data APIs (replaying relevant channel) or as normalized data type via CSV downloads.</paragraph><paragraph>collected from WS forceOrderarrow-up-right stream, since 2021-04-27 liquidation orders streams do not push realtime order data anymore, instead, they push snapshot order data at a maximum frequency of 1 order push per second</paragraph><paragraph>collected from WS forceOrderarrow-up-right stream, since 2021-04-27 liquidation orders streams do not push realtime order data anymore, instead, they push snapshot order data at a maximum frequency of 1 order push per second</paragraph><paragraph>collected by pooling OKEx REST APIs since liquidations aren't available via WS feeds</paragraph><paragraph>collected by pooling OKEx REST APIs since liquidations aren't available via WS feeds</paragraph><paragraph>up until 2021-09-20 collected by pooling Bybit REST APIs since liquidations weren't available via WS feeds, starting from 2021-09-20 collected from WS liquidationarrow-up-right channel</paragraph><paragraph>Do you provide historical options data?</paragraph><paragraph>Yes, we do provide historical options data for Deribit and OKEx Options - see options chain CSV data type and Deribit and OKEx Options exchange details pages.</paragraph><paragraph>Do you provide historical futures data?</paragraph><paragraph>We cover all leading derivatives exchanges such as BitMEX, Deribit, Binance USDT Futures, Binance COIN Futures, FTX, OKEx, Huobi Futures, Huobi Swap, Bitfinex Derivatives, Bybit and many more.</paragraph><paragraph>What is the difference between futures and perpetual swaps contracts?</paragraph><paragraph>Futures contract is a contract that has expiry date (for example quarter ahead for quarterly futures). Futures contract price converges to spot price as the contract approaches expiration/settlement date. After futures contract expires, exchange settles it and replaces with a new contract for the next period (next quarter for our previous example).</paragraph><paragraph>Perpetual swap contract also commonly called "perp", "swap", "perpetual" or "perpetual future" in crypto exchanges nomenclature is very similar to futures contract, but does not have expiry date (hence perpetual). In order to ensure that the perpetual swap contract price stays near the spot price exchanges employ mechanism called funding rate. When the funding rate is positive, Longs pay Shorts. When the funding rate is negative, Shorts pay Longs. This mechanism can be quite nuanced and vary between exchanges, so it's best to study each contract specification to learn all the details (funding periods, mark price mechanisms etc.).</paragraph><paragraph>Do you provide time based aggregated data as well?</paragraph><paragraph>We are focusing on providing the best possible tick-level historical data for cryptocurrency exchanges and as of now our APIs (both HTTP and CSV datasets) do offer access to tick-level data only and do not offer support for time based aggregated data.</paragraph><paragraph>If you're interested in time based aggregated data (OHLC, interval based order book snapshots) see our client libs that provide such capabilities, but with the caveat that data aggregation is performed client-side from tick-level data sourced from the API, meaning it can be relatively slow process in contrast to ready to download aggregated data.</paragraph><paragraph>Can you record market data for exchange that's not currently supported?</paragraph><paragraph>Yes, we're always open to support new promising exchanges. Contact usarrow-up-right and we'll get back to you to discuss the details.</paragraph><paragraph>Do you provide market data in normalized format?</paragraph><paragraph>Normalized market data (unified data format for every exchange) is available via our official libraries and downloadable CSV files. Our HTTP API provides data only in exchange-native format.</paragraph><paragraph>Do you provide normalized contract amounts for derivatives exchanges in your historical data feeds?</paragraph><paragraph>Data we provide has contract amounts exactly as provided by exchanges APIs, meaning in some cases it can be tricky to compare across exchanges due to different contract multipliers (like for example OKEx where each contract has $100 value) or different contract types (linear or inverse).</paragraph><paragraph>We'll keep it this way, but we also provide instrument metadata API that returns contract multipliers, tick sizes and more for each instrument in uniform way, allowing easily normalize the contract amounts client-side without having to go through all kinds of documentation on various exchange to find this information.</paragraph><paragraph>What is a difference between exchange-native and normalized data format?</paragraph><paragraph>Cryptocurrency markets are very fragmented and every exchange provides data in it's own bespoke data format which we call exchange-native data format**.**</paragraph><paragraph>Our HTTP API and client libs can provide market data in this format, meaning data you receive is exactly the same as the live data you would have received from exchanges ("as-is").</paragraph><paragraph>For example BitMEX trade message looks like this:</paragraph><paragraph>and this is Deribit trade message:</paragraph><paragraph>In contrast, normalized data format means the same, unified format across multiple exchanges. We provide normalized data via our client libs (data normalization is performed client-side) as well as via downloadable CSV files.</paragraph><paragraph>Sample normalized trade message:</paragraph><paragraph>We support following normalized data types via our client libs:</paragraph><list type="ul"><item>order book snapshots (tick-by-tick, 10ms, 100ms, 1s, 10s etc)</item><item>derivative tick info (open interest, funding rate, mark price, index price)</item><item>volume/tick based trade bars</item></list><paragraph>order book snapshots (tick-by-tick, 10ms, 100ms, 1s, 10s etc)</paragraph><paragraph>derivative tick info (open interest, funding rate, mark price, index price)</paragraph><paragraph>volume/tick based trade bars</paragraph><paragraph>and downloadable CSV data files:</paragraph><paragraph>What is the channel field used in the HTTP API and client libs replay functions?</paragraph><paragraph>Exchanges when publishing real-time data messages, always publish those for subscription topics clients have subscribed to. Those subscriptions topics are also very often called "channels" or "streams" in exchanges documentations pages and describe data type given message belongs to - for example BitMEX publishes it's trades data via trade channel and order book L2 updates data via orderBookL2.</paragraph><paragraph>Since we collect the data for all the channels described in exchanges' details page (Captured real-time market data channels section) our HTTP API and client libs offer filtering capability by those channels names, so for example to get historical trades for BitMEX, channel trade needs to be provided alongside requested instruments symbols (via HTTP API or client lib replay function args).</paragraph><paragraph>What time zone is used in the data?</paragraph><paragraph>UTC, always.</paragraph><paragraph>Is provided raw market data complete?</paragraph><paragraph>We're doing our best to provide the most complete and reliable historical raw data API on the market. To do so amongst many other things, we utilize highly available Kubernetes clusters on Google Cloud Platform that offer best in the class availability, networking and monitoring. However due to exchanges' APIs downtimes (maintenance, deployments, connection drops etc.) we can experience data gaps and cannot guarantee 100% data completeness, but 99.9% (99.99% on most days) which should be more than enough for most of the use cases that tick level data is useful for.</paragraph><paragraph>In rare circumstances, when exchange's API changes without any notice or we hit new unexpected rate limits we also may fail to record data during such period, it happens very rarely and is very specific for each exchange. Use /exchanges/:exchangearrow-up-right API endpoint and check for incidentReports field in order to get most detailed and up to date information on that subject.</paragraph><paragraph>How frequently exchanges drop WebSocket connections?</paragraph><paragraph>As long as exchange WebSocket API is not 'hidden' behind Cloudflare proxy (causing relatively frequent "CloudFlare WebSocket proxy restarting, Connection reset by peer" errors) connections are stable for majority of supported exchanges and there are almost no connection drops during the day. In cases when there is more volatility in the market some exchanges tend to drop connections more frequently or have larger latency spikes. Overall it's a nuanced matter that changes over time, if you'd have any questions regarding particular exchange, please do not hesitate to contact usarrow-up-right.</paragraph><paragraph>Can historical order books reconstructed from L2 updates be crossed (bid/ask overlap) occasionally?</paragraph><paragraph>Although is should never happen in theory, in practice due to various crypto exchanges bugs and peculiarities it can happen (very occasionally), see some posts from users reporting those issues:</paragraph><paragraph>We do track sequence numbers of WebSocket L2 order book messages when collecting the data and restart connection when sequence gap is detected for exchanges that do provide those numbers. We observe that even in scenario when sequence numbers are in check, bid/ask overlap can occur.</paragraph><paragraph>When such scenario occurs, exchanges tend to 'forget' to publish delete messages for the opposite side of the book when publishing new level for given side - we validated that hypothesis by comparing reconstructed order book snapshots that had crossed order book (bid/ask overlap) for which we removed order book levels for the opposite side manually (as exchange didn't publish that 'delete'), with quote/ticker feeds if best bid/ask matches (for exchanges that provide those) - see sample code that implements that manual level removal logicarrow-up-right.</paragraph><paragraph>Can exchange publish data with non monotonically increasing timestamps for single data channel?</paragraph><paragraph>That shouldn't happen in theory, but we've detected that for some exchanges when new connection is established sometimes first message for given channel & symbol has newer timestamp than subsequent message, e.g., order book snapshot has newer timestamp than first order book update. This is why we provide data via API and CSV downloads for given data ranges based on local timestamps (timestamp of message arrival) which are always monotonically increasing.</paragraph><paragraph>Are exchanges publishing duplicated trades data messages?</paragraph><paragraph>Some exchanges are occasionally publishing duplicated trades (trades with the same ids). Since we collect real-time data we also collect and provide duplicate trades via API if those were published by real-time WebSocket feeds of exchanges. Our client libraries have functionality that when working with normalized data can deduplicate such trades, similarly for downloadable CSV files we deduplicate tick-by-tick trades data.</paragraph><paragraph>How order book data snapshots are provided?</paragraph><paragraph>Historical market data available via HTTP API provides order book snapshots at the beginning of each day (00:00 UTC) - see details.</paragraph><paragraph>We also provide custom order book snapshots with customizable time intervals from tick-by-tick, milliseconds to minutes or hours via client libs in which case custom snapshots are computed client side from raw data provided via HTTP API as well as via downloadable CSV files - book_snapshot_25 and book_snapshot_5 .</paragraph><paragraph>Do you collect order books as snapshots or in streaming mode?</paragraph><paragraph>Order books are collected in streaming mode - snapshot at the beginning of each day and then incremental updates. See details.</paragraph><paragraph>We also provide custom order book snapshots with customizable time intervals from tick-by-tick, milliseconds to minutes or hours via client libs in which case custom snapshots are computed client side from raw data provided via HTTP API as well as via downloadable CSV files - book_snapshot_25 and book_snapshot_5 .</paragraph><paragraph>Cryptocurrency exchanges real-time APIs vary a lot, but for L2 order book data they all tend to follow similar flow, first when WS connection is established and subscription is confirmed, exchanges send initial order book snapshot (all existing price levels or top 'x' levels depending on exchange) and then start streaming 'book update' messages (called frequently deltas as well). Those updates when applied to initial snapshot, result in up to data order book state at given time.</paragraph><paragraph>Let's take FTX as an example and start with it's snapshot orderbook message (that is frequently called 'partial' in exchanges API docs as well).</paragraph><paragraph>Remaining bids and asks levels were removed from this sample message for the sake of clarity.</paragraph><paragraph>Such snapshot message maps to the following rows in CSV file:</paragraph><paragraph>exchange</paragraph><paragraph>symbol</paragraph><paragraph>timestamp</paragraph><paragraph>local_timestamp</paragraph><paragraph>is_snapshot</paragraph><paragraph>side</paragraph><paragraph>price</paragraph><paragraph>amount</paragraph><paragraph>... and here's a sample FTX orderbook update message.</paragraph><paragraph>Let's see how it maps to CSV format.</paragraph><paragraph>exchange</paragraph><paragraph>symbol</paragraph><paragraph>timestamp</paragraph><paragraph>local_timestamp</paragraph><paragraph>is_snapshot</paragraph><paragraph>side</paragraph><paragraph>price</paragraph><paragraph>amount</paragraph><paragraph>See this answer if you have doubts how to reconstruct order book state based on data provided in incremental_book_L2 dataset.</paragraph><heading level="2">How can I reconstruct full order book state from incremental_book_L2 CSV dataset?</heading><paragraph>How can I reconstruct full order book state from</paragraph><paragraph>CSV dataset?</paragraph><paragraph>In order to reconstruct full order book state correctly from incremental_book_L2 data:</paragraph><list type="ul"><item>For each row in the CSV file (iterate in the same order as provided in file):only if local timestamp of current row is larger than previous row local timestamp(local_timestamp column value) it means you can read your local order book state as it's consistent, why? CSV format is flat where each row represents single price level update, but most exchanges real-time feeds publish multiple order book levels updates via single WebSocket message that need to be processed together before reading locally maintained order book state. We use local timestamp value here to detect all price level updates belonging to single 'update' message.if current row is a part of the snapshot (is_snapshot column value set to true) and previous one was not, reset your local order book state object that tracks price levels for each order book side as it means that there was a connection restart and exchange provided full order book snapshot or it was a start of a new day (each incremental_book_L2 file starts with the snapshot)if current row amount is set to zero (amount column value set to 0) remove such price level (row's price column) from your local order book state as such price level does not exist anymoreif current row amount is not set to zero update your local order book state price level with new value or add new price level if not exist yet in your local order book state - maintain separately bids and asks order book sides (side column value)</item></list><paragraph>For each row in the CSV file (iterate in the same order as provided in file):</paragraph><list type="ul"><item>only if local timestamp of current row is larger than previous row local timestamp(local_timestamp column value) it means you can read your local order book state as it's consistent, why? CSV format is flat where each row represents single price level update, but most exchanges real-time feeds publish multiple order book levels updates via single WebSocket message that need to be processed together before reading locally maintained order book state. We use local timestamp value here to detect all price level updates belonging to single 'update' message.</item><item>if current row is a part of the snapshot (is_snapshot column value set to true) and previous one was not, reset your local order book state object that tracks price levels for each order book side as it means that there was a connection restart and exchange provided full order book snapshot or it was a start of a new day (each incremental_book_L2 file starts with the snapshot)</item><item>if current row amount is set to zero (amount column value set to 0) remove such price level (row's price column) from your local order book state as such price level does not exist anymore</item><item>if current row amount is not set to zero update your local order book state price level with new value or add new price level if not exist yet in your local order book state - maintain separately bids and asks order book sides (side column value)</item></list><paragraph>only if local timestamp of current row is larger than previous row local timestamp(local_timestamp column value) it means you can read your local order book state as it's consistent, why? CSV format is flat where each row represents single price level update, but most exchanges real-time feeds publish multiple order book levels updates via single WebSocket message that need to be processed together before reading locally maintained order book state. We use local timestamp value here to detect all price level updates belonging to single 'update' message.</paragraph><paragraph>if current row is a part of the snapshot (is_snapshot column value set to true) and previous one was not, reset your local order book state object that tracks price levels for each order book side as it means that there was a connection restart and exchange provided full order book snapshot or it was a start of a new day (each incremental_book_L2 file starts with the snapshot)</paragraph><paragraph>if current row amount is set to zero (amount column value set to 0) remove such price level (row's price column) from your local order book state as such price level does not exist anymore</paragraph><paragraph>if current row amount is not set to zero update your local order book state price level with new value or add new price level if not exist yet in your local order book state - maintain separately bids and asks order book sides (side column value)</paragraph><paragraph>Alternatively we do also provide top 25 and top 5 levels order book snapshots CSV datasets ready to download.</paragraph><paragraph>How CSV datasets are split into the files?</paragraph><paragraph>CSV datasets are available in daily intervals split by exchange, data type and symbol. In addition to standard currency pairs/instrument symbols, each exchange also has special 'grouped' symbols available depending if it supports given market type: SPOT, FUTURES, OPTIONS and PERPETUALS. That feature is useful if someone is interested in for examples all Deribit's options instruments' trades or quotes data without a need to request data for each symbol separately one by one.</paragraph><paragraph>How market data messages are being timestamped?</paragraph><paragraph>Each message received via WebSocket connection is timestamped with 100ns precision using synchronized clockarrow-up-right at arrival time (before any message processing) and stored in ISO 8601 format.</paragraph><paragraph>What is the new historical market data delay in relation to real-time?</paragraph><paragraph>For API access it's 15 minutes (T - 15min), downloadable CSV files for given day are available on the next day around 06:00 UTC.</paragraph></content>
</page>
<page url="https://docs.tardis.dev/faq/general">
<title>General | Tardis.dev Documentation</title>
<content><paragraph>What is Tardis.dev and what is your unique value proposition?</paragraph><paragraph>Tardis.dev provides the most comprehensive and granular cryptocurrency market data products in the industry and offers:</paragraph><paragraph>Data use cases</paragraph><list type="ul"><item>market microstructure and order book dynamics research</item><item>trading execution optimization</item><item>tick-level granularity market simulation</item><item>liquidity and lead-lag analysis</item><item>backtesting and optimization of trading strategies</item><item>full historical order book reconstruction at any given point in time</item><item>training machine learning models</item><item>designing quantitative models</item></list><paragraph>market microstructure and order book dynamics research</paragraph><paragraph>trading execution optimization</paragraph><paragraph>tick-level granularity market simulation</paragraph><paragraph>liquidity and lead-lag analysis</paragraph><paragraph>backtesting and optimization of trading strategies</paragraph><paragraph>full historical order book reconstruction at any given point in time</paragraph><paragraph>training machine learning models</paragraph><paragraph>designing quantitative models</paragraph><paragraph>Which exchanges, instruments and currency pairs are supported?</paragraph><paragraph>In total over 40 000 distinct instruments & currency pairs across leading derivatives and spot cryptocurrency exchanges is supported. We collect and provide data for all instruments & currency pairs available on given exchange with some exceptions for spot exchanges where we collect high caps currency pairs only (due to exchanges API limitations).</paragraph><paragraph>Do you provide discounts?</paragraph><paragraph>We do provide discounts in a transparent form via different subscriptions types.</paragraph><paragraph>Do you offer free trials?</paragraph><paragraph>Yes, if you'd like to test the service (data quality, coverage, API performance etc.) we offer generous free trials. Simply reach out to usarrow-up-right and we'll set up trial account for you.</paragraph><paragraph>What does professional support mean?</paragraph><paragraph>Our support team has in-depth knowledge of market data and exchanges' APIs peculiarities, programming and data analysis expertise. You get the answers straight from people whose day to day job is overseeing and maintaining market data collection process and infrastructure.</paragraph><paragraph>For business subscriptions we provide dedicated communication channel (via Telegram Messenger, email or Zoom calls) for your company where our team is on standby every business day (7AM - 3PM UTC) to answer any questions you may have.</paragraph><paragraph>For pro subscriptions and one-off purchases we do provide email based support with 24-48 business hours initial response time.</paragraph><paragraph>For academic and solo subscriptions there is no dedicated support provided, only self-service.</paragraph><paragraph>What would I use your services if I can collect data by myself?</paragraph><paragraph>Since cryptocurrency exchanges' market data APIs are public anyone can use those to collect the data, but it's a time consuming and resource intensive undertaking (exchanges we support publish ~1000GB of new data every day), that requires investment in proper infrastructure, constant monitoring and oversight (exchanges API changes, rate-limiting monitoring, new exchanges integrations, unexpected connection issues, latency monitoring etc.), not to mention implementation costs of data collection, storage and distribution services.</paragraph><paragraph>All in all we think our offering is comprehensive, transparent and fair, provides good value and saves you time and money in comparison to in-house solution allowing you to focus on your core objective not on data management intricacies.</paragraph><paragraph>How can I download the data?</paragraph><paragraph>You can access historical market data via API which provides raw data in exchange native format or download CSV datasets with trades, incremental order book L2 updates, order book snapshots, options chains, quotes, derivative tickers (open interest, funding, mark price, index price) and liquidations.</paragraph><paragraph>Our client libs provide data in normalized format as well, which can be more flexible than CSV datasets for some use cases but also slower to download due to on-demand, client-side data normalization overhead in comparison to ready to download CSV files.</paragraph><paragraph>How far back the historical data is available?</paragraph><paragraph>Data is available since 2019-03-30 for majority of the supported exchanges (that existed at that time).</paragraph><paragraph>Do you provide historical market data in CSV flat files?</paragraph><paragraph>Yes, see downloadable CSV files documentation for more details.</paragraph><paragraph>What programming languages are supported?</paragraph><paragraph>Any programming language that can communicate using HTTPS can communicate with our HTTP API.</paragraph><paragraph>We do provide official Python and Node.js clients that offer fast and convenient access to tick-level historical market data.</paragraph><paragraph>Finally, our open source, locally runnable tardis-machine server with built-in local data caching, provides market data normalization, custom order book snapshots capabilities and real-time market data streaming support that connects directly to exchanges' WebSocket APIs. It provides both streaming HTTP and WebSocket endpoints returning market data for whole time periods (in contrast to Tardis.dev HTTP API where single call returns data for single minute time period) and is available via npm and as a Docker Image.</paragraph><paragraph>What API protocols can be used to access market data?</paragraph><paragraph>Historical market data provided by HTTP API can be accessed via HTTPS.</paragraph><paragraph>Locally runnable tardis-machine server provides both HTTP and WebSocket based APIs for accessing both historical and real-time market data.</paragraph><paragraph>How time-machine market replay works?</paragraph><paragraph>Exchanges' market data WebSocket APIs are designed to publish real-time feeds and not historical ones. Locally runnable tardis-machine server's WebSocket API bridges that gap and allows "replaying" historical market data from any given past point in time with the same data format and 'subscribe' logic as real-time exchanges' APIs. In many cases existing exchanges' WebSocket clients can be used to connect to this endpoint just by changing URL, and receive market data in exchange-native format for date ranges specified in URL query string params.</paragraph><paragraph>Do you support consolidated real-time market data streaming?</paragraph><paragraph>We do not provide hosted real-time market data API as we think that given everyone can access exchanges' APIs directly for free without restrictions, relaying on 3rd party for such crucial piece of infrastructure does not make sense (additional latency and another SPOFarrow-up-right).</paragraph><paragraph>Instead we developed locally runnable (self hosted) server and open source libraries that offer consolidated real-time normalized market data streaming capabilities, connect directly to exchanges' WebSocket APIs and are completely free to use.</paragraph><paragraph>Are there any rate-limits for the API?</paragraph><paragraph>There are no API rate limits for downloadable CSV files API.</paragraph><paragraph>Raw data replay API for professional level subscriptions is limited to 30 millions requests per day and up to 60 concurrent requests. API key can be used only from single IP adress at the same time.</paragraph><paragraph>For business level subscriptions there are no rate limits for raw data replay API as long as your behavior does not negatively impact other customers API usage experience. If that's the case, we'll contact your via email and do our best to help how to sort it out - in most cases it's download client bug that over and over downloads the same data in a loop.</paragraph><paragraph>How do I obtain my API key?</paragraph><paragraph>API key can be obtained on Tardis.devarrow-up-right website via order formarrow-up-right. You'll receive it via email after successful order.</paragraph><paragraph>What if my API key was compromised?</paragraph><paragraph>Contact usarrow-up-right immediately and we will generate new API key for you.</paragraph><paragraph>What is your infrastructure setup?</paragraph><paragraph>Market data collection</paragraph><paragraph>Highly available Google Cloud Platform Kubernetes Clustersarrow-up-right located in in London, UK (europe-west2 region) and Tokyo, Japan (asia-northeast1 region)</paragraph><paragraph>Market data storage</paragraph><paragraph>Two independent, geo-redundant, highly durable storage services</paragraph><paragraph>Market data distribution</paragraph><paragraph>High performance API servers deployed across network of data centers around the globe</paragraph><paragraph>Premium connectivity (Pro & Business subscriptions)</paragraph><paragraph>For Pro and Business subscriptions, we provide premium connectivity powered by Cloudflare Argo Smart Routingarrow-up-right. Argo Smart Routing optimizes the network path for API requests by routing traffic through Cloudflare's private network backbone instead of the congested public internet.</paragraph><paragraph>Benefits include:</paragraph><list type="ul"><item>Reduced latency - Argo analyzes and optimizes routing decisions in real-time, finding the fastest path across Cloudflare's global network</item><item>Improved reliability - Automatic failover around network congestion, packet loss, and outages</item><item>Optimized historical data API performance - Faster data retrieval for large historical data requests through intelligent routing</item><item>Consistent performance - More predictable response times regardless of geographic location</item></list><paragraph>Reduced latency - Argo analyzes and optimizes routing decisions in real-time, finding the fastest path across Cloudflare's global network</paragraph><paragraph>Improved reliability - Automatic failover around network congestion, packet loss, and outages</paragraph><paragraph>Optimized historical data API performance - Faster data retrieval for large historical data requests through intelligent routing</paragraph><paragraph>Consistent performance - More predictable response times regardless of geographic location</paragraph><paragraph>Do you provide SLA?</paragraph><paragraph>We do not have a formal SLA in place yet, but all infrastructure is set up to provide highest availability possible on both data collection and distribution side with geo-redundant setup. Both data collection services and public APIs are constantly monitored from multiple locations and our team is immediately notified in case of any issue. We don't practice maintenance that would affect API availability, but in very rare circumstance if that would happen we'll communicate that in advance. If a formal SLA is something that your business require contact usarrow-up-right.</paragraph></content>
</page>
<page url="https://docs.tardis.dev/faq/billing-and-subscriptions">
<title>Billing and Subscriptions | Tardis.dev Documentation</title>
<content><paragraph>What is the order process?</paragraph><list type="ol"><item>Proceed to checkout where you provide email address and payment details.arrow-up-rightaccepted payment methodsCredit Cards (Mastercard Visa Maestro American Express Discover Diners Club JCB UnionPay)Apple Pay (one-off purchases only)Wire Transfers (for one-off purchases only)</item></list><paragraph>Proceed to checkout where you provide email address and payment details.arrow-up-right</paragraph><list type="ul"><item>accepted payment methodsCredit Cards (Mastercard Visa Maestro American Express Discover Diners Club JCB UnionPay)Apple Pay (one-off purchases only)Wire Transfers (for one-off purchases only)</item></list><paragraph>accepted payment methods</paragraph><list type="ul"><item>Credit Cards (Mastercard Visa Maestro American Express Discover Diners Club JCB UnionPay)</item><item>Apple Pay (one-off purchases only)</item><item>Wire Transfers (for one-off purchases only)</item></list><paragraph>Credit Cards (Mastercard Visa Maestro American Express Discover Diners Club JCB UnionPay)</paragraph><paragraph>Apple Pay (one-off purchases only)</paragraph><paragraph>Wire Transfers (for one-off purchases only)</paragraph><paragraph>Do you provide discounts?</paragraph><paragraph>We do provide discounts in a transparent form via different subscriptions types.</paragraph><paragraph>How one-off purchase based access works?</paragraph><paragraph>One-off purchase provides access to specific time periods of historical market data. API key is valid for a year since purchase and allows access to all available data types (trades, order books etc.) for ordered date ranges both via API and downloadable CSV files.</paragraph><paragraph>How subscription based access works?</paragraph><paragraph>Subscriptions based access model relies on recurring payments at regular intervals (monthly, quarterly, yearly) and offers access to newly collected market data as it becomes available as well as existing historical market data which range depends on chosen billing period.</paragraph><paragraph>There are three 'dimensions' you can customize your subscription by:</paragraph><paragraph>For example "All Exchanges" Business Subscription with yearly billing period allows accessing all available existing historical data via API and CSV files and one year of new data as it becomes available (for initial payment).</paragraph><paragraph>API key is valid as long as subscription is active and allows access to all available data types (trades, orders book data, quotes, funding, liquidations etc.) via downloadable CSV files and via raw market data API (for Pro and Business subscriptions types only).</paragraph><paragraph>What are the differences between subscriptions types?</paragraph><paragraph>Do subscriptions include access to historical data as well?</paragraph><paragraph>Yes, depending on chosen billing period, subscriptions include access to existing historical market data as well:</paragraph><list type="ul"><item>all available historical data if subscription is billed yearly - historical market data is available since 2019-03-30 for majority of the supported exchanges (see exchange details page for exact date for particular exchange)</item><item>12 months of historical data if subscription is billed quarterly, e.g., subscription that has started at 2020-04-01, includes access to historical data since 2019-04-01 - it's not a rolling time window, but fixed starting date since when historical data is available for your subscription</item><item>4 months of historical data if subscription is billed monthly, e.g., subscription that has started at 2020-04-01, includes access to historical data since 2019-12-01 - it's not a rolling time window, but fixed starting date since when historical data is available for your subscription</item></list><paragraph>all available historical data if subscription is billed yearly - historical market data is available since 2019-03-30 for majority of the supported exchanges (see exchange details page for exact date for particular exchange)</paragraph><paragraph>12 months of historical data if subscription is billed quarterly, e.g., subscription that has started at 2020-04-01, includes access to historical data since 2019-04-01 - it's not a rolling time window, but fixed starting date since when historical data is available for your subscription</paragraph><paragraph>4 months of historical data if subscription is billed monthly, e.g., subscription that has started at 2020-04-01, includes access to historical data since 2019-12-01 - it's not a rolling time window, but fixed starting date since when historical data is available for your subscription</paragraph><paragraph>All subscriptions provide access to all available data types (trades, orders book data, quotes, funding etc.) via downloadable CSV files and raw data replay API (for pro and business subscriptions types).</paragraph><paragraph>What is included in "Individual" data plan?</paragraph><paragraph>"Individual" data plan provides per-exchange access to market data that includes full feed (all instruments) and data types of selected exchange(s), for example full Coinbase exchange data feed.</paragraph><paragraph>"Individual" data plan allows access to all available data types (trades, orders book data, quotes, funding etc.) via downloadable CSV files and raw data replay API (for pro and business subscriptions types).</paragraph><paragraph>Range of historical data access for "Individual" data plan depends on chosen billing period (for example: access to all existing historical data we collected if subscription is billed yearly).</paragraph><paragraph>What is included in "Perpetuals" data plan?</paragraph><paragraph>"Perpetuals" data plan provides access to the following perpetual swaps instruments' market data (over 500 perpetual swaps instruments across 13 exchanges):</paragraph><list type="ul"><item>BitMEX: all perpetual swaps instruments</item><item>Deribit: all perpetual swaps instruments</item><item>FTX: all perpetual swaps instruments</item><item>OKX Swap: all perpetual swaps instruments</item><item>Bybit: all perpetual swaps instruments</item><item>dYdX: all perpetual swaps instruments</item><item>Phemex: all perpetual swaps instruments</item><item>Delta: all perpetual swaps instruments</item><item>CoinFLEX: all perpetual swaps instruments</item><item>dYdX: all perpetual swaps instruments</item><item>WOO X: all perpetual swaps instruments</item><item>Ascendex: all perpetual swaps instruments</item></list><paragraph>BitMEX: all perpetual swaps instruments</paragraph><paragraph>Deribit: all perpetual swaps instruments</paragraph><paragraph>FTX: all perpetual swaps instruments</paragraph><paragraph>OKX Swap: all perpetual swaps instruments</paragraph><paragraph>Bybit: all perpetual swaps instruments</paragraph><paragraph>dYdX: all perpetual swaps instruments</paragraph><paragraph>Phemex: all perpetual swaps instruments</paragraph><paragraph>Delta: all perpetual swaps instruments</paragraph><paragraph>CoinFLEX: all perpetual swaps instruments</paragraph><paragraph>dYdX: all perpetual swaps instruments</paragraph><paragraph>WOO X: all perpetual swaps instruments</paragraph><paragraph>Ascendex: all perpetual swaps instruments</paragraph><paragraph>"Perpetuals" data plan allows access to all available data types (trades, orders book data, funding etc.) via downloadable CSV files and raw data replay API (for pro and business subscriptions types).</paragraph><paragraph>Range of historical data access for "Perpetuals" data plan depends on chosen billing period (for example: access to all existing historical data we collected if subscription is billed yearly).</paragraph><paragraph>What is included in "Derivatives" data plan?</paragraph><paragraph>"Derivatives" data plan provides access to the following derivatives exchanges' market data:</paragraph><list type="ul"><item>BitMEX: all exchange's instruments</item><item>Deribit: all exchange's instruments</item><item>FTX: all exchange's instruments</item><item>Bybit: all exchange's instruments</item><item>DYdX: all exchange's instruments</item><item>Phemex: all exchange's instruments</item><item>Delta: all exchange's instruments</item><item>dYdX: all perpetual swaps instruments</item><item>WOO X: all exchange's instruments</item></list><paragraph>BitMEX: all exchange's instruments</paragraph><paragraph>Deribit: all exchange's instruments</paragraph><paragraph>FTX: all exchange's instruments</paragraph><paragraph>Bybit: all exchange's instruments</paragraph><paragraph>DYdX: all exchange's instruments</paragraph><paragraph>Phemex: all exchange's instruments</paragraph><paragraph>Delta: all exchange's instruments</paragraph><paragraph>dYdX: all perpetual swaps instruments</paragraph><paragraph>WOO X: all exchange's instruments</paragraph><paragraph>"Derivatives" data plan allows access to all available data types (trades, orders book data, quotes, funding etc.) via downloadable CSV files and raw data replay API (for pro and business subscriptions types).</paragraph><paragraph>Range of historical data access for "Derivatives" data plan depends on chosen billing period (for example: access to all existing historical data we collected if subscription is billed yearly).</paragraph><paragraph>What is included in "All Exchanges" data plan?</paragraph><paragraph>"All Exchanges" data plan provides access to market data of all supported exchanges (30+ leading spot and derivatives exchanges, see full list).</paragraph><paragraph>"All Exchanges" data plan allows access to all available data types (trades, orders book data, quotes, funding, liquidations etc.) for all supported exchanges and theirs instruments/currency pairs via downloadable CSV files and raw data replay API (for pro and business subscriptions types).</paragraph><paragraph>Range of historical data access for "All Exchanges" data plan depends on chosen billing period (for example: access to all existing historical data we collected if subscription is billed yearly).</paragraph><paragraph>How can I change my subscription plan?</paragraph><paragraph>Contact usarrow-up-right describing which plan you'd like to change to and we'll handle the rest.</paragraph><paragraph>Can I pay through invoicing?</paragraph><paragraph>We offer invoicing for customers paying over $6000 for data access. Simply use our order formarrow-up-right and "PAY THROUGH INVOICING" button.</paragraph><paragraph>Alternatively contact usarrow-up-right with orders details you're interested in (data plan, billing period) and we'll send you back invoice that if paid will give you the access to the data.</paragraph><paragraph>Can I get quotation document before making an order?</paragraph><paragraph>Yes, please use our order formarrow-up-right and "REQUEST QUOTATION" button.</paragraph><paragraph>Alternatively contact usarrow-up-right with orders details you're interested in (data plan, billing period) and we'll send you back quotation document in no time.</paragraph><paragraph>How can I get an invoice and VAT refund?</paragraph><paragraph>After successful order you'll receive Receipt email from Paddlearrow-up-right which is our online reseller & payment processor. Click on the button titled "View Receipt" there.</paragraph><paragraph>You will be redirected to the receipt page where you will be able to enter your address details by clicking on "Add address & VAT Number" link.</paragraph><paragraph>If you would like to enter a VAT number select "This is a business purchase" checkbox to enter the VAT ID if forgot to enter it during the checkout. The tax amount will be refunded in max. 12 hours after it is confirmed by Paddle.</paragraph><list type="ul"><item>Right click on the screen and click 'Print...' in context menu</item><item>Change destination to 'Save as PDF'</item><item>Click 'Save' button to save invoice as PDF file</item></list><paragraph>Right click on the screen and click 'Print...' in context menu</paragraph><paragraph>Change destination to 'Save as PDF'</paragraph><paragraph>Click 'Save' button to save invoice as PDF file</paragraph><paragraph>What are your VAT details?</paragraph><paragraph>Click on the link titled "Click here to get a full invoice with address & custom information" provided with the order confirmation email sent by Paddle to get the address and VAT ID of Paddle who process our payments. Paddle acts as a reseller and Merchant of Record so they handle VAT on our behalf.</paragraph><paragraph>I've lost my invoice. How do I get a new one?</paragraph><paragraph>You need to contact usarrow-up-right or [email protected] to request a new invoice. Please provide the email address you bought the subscription with and any extra details that might help.</paragraph><paragraph>What is the refund policy?</paragraph><paragraph>We do not offer refunds for initial subscription payments and one-off purchases.</paragraph><paragraph>If you are on yearly billing and forget to cancel your subscription before the renewal date, reach out to usarrow-up-right within seven days after the renewal date to discuss a refund.</paragraph><paragraph>If you're on a monthly or quarterly billing, please be sure to cancel your subscription before the end date of your current plan as there are no refunds for recurring payments on monthly and quarterly billing plans.</paragraph><paragraph>If you'd like to test the service, we offer generous free trials. Simply reach out to usarrow-up-right and we'll set up test account for you in no time.</paragraph><paragraph>How can I cancel my subscription?</paragraph><paragraph>In order to cancel you active subscription use the 'Cancel subscription' link we've sent you in email together with your API key or contact usarrow-up-right and we'll provide cancellation link for you.</paragraph><paragraph>Alternatively you can email Paddlearrow-up-right ([email protected]) which acts as our reseller and Merchant of Record including a note of the email address you used to purchase your subscription and your order number.</paragraph><paragraph>Do you accept payments in cryptocurrency?</paragraph><paragraph>We accept BTC, ETH and USDT for one-off purchases. Contact usarrow-up-right and we'll get back to you with details.</paragraph><paragraph>How I can update my credit card information?</paragraph><paragraph>In order to update your credit card information use the 'Update payment method' link we've sent you in email together with your API key or contact usarrow-up-right and we'll provide that link for you.</paragraph></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/deribit">
<title>Deribit | Tardis.dev Documentation</title>
<content><code language="python"># requires Python >=3.6
# pip install tardis-dev

from tardis_dev import datasets, get_exchange_details
import logging

# optionally enable debug logs
# logging.basicConfig(level=logging.DEBUG)

exchange = 'deribit'
exchange_details = get_exchange_details(exchange)   

# iterate over and download all data for every symbol
for symbol in exchange_details["datasets"]["symbols"]:
    # alternatively specify datatypes explicitly ['trades', 'incremental_book_L2', 'quotes'] etc
    # see available options https://docs.tardis.dev/downloadable-csv-files#data-types
    data_types = symbol["dataTypes"]  
    symbol_id = symbol["id"]
    from_date =  symbol["availableSince"]
    to_date = symbol["availableTo"]

    # skip groupped symbols
    if symbol_id in ['PERPETUALS', 'SPOT', 'FUTURES']:
        continue

    print(f"Downloading {exchange} {data_types} for {symbol_id} from {from_date} to {to_date}")

    # each CSV dataset format is documented at https://docs.tardis.dev/downloadable-csv-files#data-types
    # see https://docs.tardis.dev/downloadable-csv-files#download-via-client-libraries for full options docs
    datasets.download(
        exchange = exchange,
        data_types = data_types,
        from_date =  from_date,
        to_date = to_date,
        symbols = [symbol_id],
        # TODO set your API key here
        api_key = "YOUR_API_KEY",
        # path where CSV data will be downloaded into
        download_dir = "./datasets",
    )</code></content>
</page>
<page url="https://docs.tardis.dev/cdn-cgi/l/email-protection">
<title>Email Protection | Cloudflare</title>
<content><paragraph>The website from which you got to this page is protected by Cloudflare. Email addresses on that page have been hidden in order to keep them from being accessed by malicious bots. You must enable Javascript in your browser in order to decode the e-mail address.</paragraph><paragraph>If you have a website and are interested in protecting it in a similar way, you can sign up for Cloudflare.</paragraph></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/okex-options">
<title>OKX Options | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'okex-options',
      from: '2020-02-01',
      to: '2020-02-02',
      filters: [{ channel: 'option/trade', symbols: [] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by OKEx real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/poloniex">
<title>Poloniex | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'poloniex',
      from: '2020-07-01',
      to: '2020-07-02',
      filters: [{ channel: 'price_aggregated_book', symbols: ['USDT_BTC'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Poloniex real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/api">
<title>Getting Started | Tardis.dev Documentation</title>
<content><list type="ol"><item>API</item></list><heading level="1">Getting Started</heading><paragraph>Overview of the main ways Tardis.dev historical market data can be accessed programmatically</paragraph></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/binance">
<title>Binance Spot | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'binance',
      from: '2019-12-01',
      to: '2019-12-02',
      filters: [{ channel: 'depth', symbols: ['btcusdt'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Binance real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/binance-futures">
<title>Binance USDT Futures | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'binance-futures',
      from: '2020-02-01',
      to: '2020-02-02',
      filters: [{ channel: 'depth', symbols: ['btcusdt'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Binance USDT Futures real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/bitmex">
<title>BitMEX | Tardis.dev Documentation</title>
<content><code language="python"># requires Python >=3.6
# pip install tardis-dev

from tardis_dev import datasets, get_exchange_details
import logging

# optionally enable debug logs
# logging.basicConfig(level=logging.DEBUG)

exchange = 'bitmex'
exchange_details = get_exchange_details(exchange)   

# iterate over and download all data for every symbol
for symbol in exchange_details["datasets"]["symbols"]:
    # alternatively specify datatypes explicitly ['trades', 'incremental_book_L2', 'quotes'] etc
    # see available options https://docs.tardis.dev/downloadable-csv-files#data-types
    data_types = symbol["dataTypes"]  
    symbol_id = symbol["id"]
    from_date =  symbol["availableSince"]
    to_date = symbol["availableTo"]

    # skip groupped symbols
    if symbol_id in ['PERPETUALS', 'SPOT', 'FUTURES']:
        continue

    print(f"Downloading {exchange} {data_types} for {symbol_id} from {from_date} to {to_date}")

    # each CSV dataset format is documented at https://docs.tardis.dev/downloadable-csv-files#data-types
    # see https://docs.tardis.dev/downloadable-csv-files#download-via-client-libraries for full options docs
    datasets.download(
        exchange = exchange,
        data_types = data_types,
        from_date =  from_date,
        to_date = to_date,
        symbols = [symbol_id],
        # TODO set your API key here
        api_key = "YOUR_API_KEY",
        # path where CSV data will be downloaded into
        download_dir = "./datasets",
    )</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/ftx">
<title>FTX | Tardis.dev Documentation</title>
<content><code language="python"># requires Python >=3.6
# pip install tardis-dev

from tardis_dev import datasets, get_exchange_details
import logging

# optionally enable debug logs
# logging.basicConfig(level=logging.DEBUG)

exchange = 'ftx'
exchange_details = get_exchange_details(exchange)   

# iterate over and download all data for every symbol
for symbol in exchange_details["datasets"]["symbols"]:
    # alternatively specify datatypes explicitly ['trades', 'incremental_book_L2', 'quotes'] etc
    # see available options https://docs.tardis.dev/downloadable-csv-files#data-types
    data_types = symbol["dataTypes"]  
    symbol_id = symbol["id"]
    from_date =  symbol["availableSince"]
    to_date = symbol["availableTo"]

    # skip groupped symbols
    if symbol_id in ['PERPETUALS', 'SPOT', 'FUTURES']:
        continue

    print(f"Downloading {exchange} {data_types} for {symbol_id} from {from_date} to {to_date}")

    # each CSV dataset format is documented at https://docs.tardis.dev/downloadable-csv-files#data-types
    # see https://docs.tardis.dev/downloadable-csv-files#download-via-client-libraries for full options docs
    datasets.download(
        exchange = exchange,
        data_types = data_types,
        from_date =  from_date,
        to_date = to_date,
        symbols = [symbol_id],
        # TODO set your API key here
        api_key = "YOUR_API_KEY",
        # path where CSV data will be downloaded into
        download_dir = "./datasets",
    )</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/okex">
<title>OKX Spot | Tardis.dev Documentation</title>
<content><code language="python"># requires Python >=3.6
# pip install tardis-dev

from tardis_dev import datasets, get_exchange_details
import logging

# optionally enable debug logs
# logging.basicConfig(level=logging.DEBUG)

exchange = 'okex'
exchange_details = get_exchange_details(exchange)   

# iterate over and download all data for every symbol
for symbol in exchange_details["datasets"]["symbols"]:
    # alternatively specify datatypes explicitly ['trades', 'incremental_book_L2', 'quotes'] etc
    # see available options https://docs.tardis.dev/downloadable-csv-files#data-types
    data_types = symbol["dataTypes"]  
    symbol_id = symbol["id"]
    from_date =  symbol["availableSince"]
    to_date = symbol["availableTo"]

    # skip groupped symbols
    if symbol_id in ['PERPETUALS', 'SPOT', 'FUTURES']:
        continue

    print(f"Downloading {exchange} {data_types} for {symbol_id} from {from_date} to {to_date}")

    # each CSV dataset format is documented at https://docs.tardis.dev/downloadable-csv-files#data-types
    # see https://docs.tardis.dev/downloadable-csv-files#download-via-client-libraries for full options docs
    datasets.download(
        exchange = exchange,
        data_types = data_types,
        from_date =  from_date,
        to_date = to_date,
        symbols = [symbol_id],
        # TODO set your API key here
        api_key = "YOUR_API_KEY",
        # path where CSV data will be downloaded into
        download_dir = "./datasets",
    )</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/bitflyer">
<title>bitFlyer | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'bitflyer',
      from: '2020-01-01',
      to: '2020-01-02',
      filters: [{ channel: 'lightning_executions', symbols: ['FX_BTC_JPY'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by bitFlyer real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/huobi-dm">
<title>Huobi Futures | Tardis.dev Documentation</title>
<content><list type="ol"><item>Historical Data Details</item></list><heading level="1">Huobi Futures</heading><paragraph>Huobi Futures historical market data details - instruments, data coverage and data collection specifics</paragraph><paragraph>Huobi Futures historical data for all it's instruments is available since 2019-11-19.</paragraph><paragraph>Downloadable CSV files</paragraph><paragraph>Historical CSV datasets for the first day of each month are available to download without API key. See downloadable CSV files documentation.</paragraph><paragraph>API Access and data format</paragraph><paragraph>Historical data format is the same as provided by real-time Huobi Futures WebSocket API with addition of local timestamps. If you'd like to work with normalized data format instead (same format for each exchange) see downloadable CSV files or official client libs that perform data normalization client-side.</paragraph><paragraph>Python</paragraph><paragraph>Node.js</paragraph><paragraph>cURL & HTTP API</paragraph><paragraph>cURL & tardis-machine</paragraph><paragraph>See HTTP API docs.</paragraph><paragraph>Tardis-machine is a locally runnable server that exposes API allowing efficiently requesting historical market data for whole time periods in contrast to HTTP API that provides data only in minute by minute slices.</paragraph><paragraph>See tardis-machine docs.</paragraph><paragraph>Captured real-time channels</paragraph><list type="ul"><item>deptharrow-up-right During data collection integrity of order book incremental updates is being validated using sequence numbersarrow-up-right provided by Huobi Futures real-time feed (version field) - in case of detecting missed message WebSocket connection is being restarted. See also details below regarding depth channel data collection details.</item></list><paragraph>deptharrow-up-right During data collection integrity of order book incremental updates is being validated using sequence numbersarrow-up-right provided by Huobi Futures real-time feed (version field) - in case of detecting missed message WebSocket connection is being restarted. See also details below regarding depth channel data collection details.</paragraph><paragraph>Up until 2020-01-31 depth channel was collected with step0 aggregation level (no aggregation) which produces full order book snapshots for each book change which is very inefficient to store. To circumvent this issue we stored only initial book snapshots and then incremental updates instead - incremental updates were calculated by diffing two subsequent book snapshots and provided in the same format as other depth messages, except having additional update: true flag set as in snippet below. Update with amount (second value in array) set to 0 means such level should be deleted, otherwise price level should be updated with new amount value.</paragraph><paragraph>On 2020-01-31 we've switched to depth.size_150.high_freq channel instead when collecting data and which natively provides incremental order book updates without workarounds described above.</paragraph><paragraph>Unfortunately it means that when requesting data for depth channel it may return slightly different format depending for which time period request was made. It's only slightly different and boils down to the way order book update messages are marked vs order book snapshots. In depth.size_150.high_freq order book message has event field always present with value update or snapshot, for example:</paragraph><paragraph>For messages before 2020-01-31 we've used depth.step0 channel for collecting order book data which means order book update message has flag update set to true, if it's a snapshot it doesn't have that flag at all, for example:</paragraph><paragraph>All other fields are are the same (tick.bids and tick.asks etc).</paragraph><paragraph>Please feel free to contact usarrow-up-right if it's confusing in any way.</paragraph><paragraph>We also provide normalization layer that handles those differences transparently via our client libs.</paragraph><paragraph>Market data collection details</paragraph><paragraph>Market data collection infrastructure for Huobi Futures since 2020-06-19 is located in GCP asia-northeast1 (Tokyo, Japan), before that it was located in GCP europe-west2 region (London, UK).</paragraph><paragraph>Real-time market data is captured via multiple WebSocket connections.</paragraph></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/bitstamp">
<title>Bitstamp | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'bitstamp',
      from: '2019-07-01',
      to: '2019-07-02',
      filters: [{ channel: 'diff_order_book', symbols: ['btcusd'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Bitstamp real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/coinbase">
<title>Coinbase Pro | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'coinbase',
      from: '2019-07-01',
      to: '2019-07-02',
      filters: [{ channel: 'l2update', symbols: ['BTC-USD'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Coinbase Pro real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/cryptofacilities">
<title>Kraken Futures | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'cryptofacilities',
      from: '2019-07-01',
      to: '2019-07-02',
      filters: [{ channel: 'book', symbols: ['PI_XBTUSD'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Kraken Futures real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/gemini">
<title>Gemini | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'gemini',
      from: '2020-01-01',
      to: '2020-01-02',
      filters: [{ channel: 'l2_updates', symbols: ['BTCUSD'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Gemini real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/kraken">
<title>Kraken | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'kraken',
      from: '2019-07-01',
      to: '2019-07-02',
      filters: [{ channel: 'book', symbols: ['XBT/USD'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Kraken real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/bitfinex">
<title>Bitfinex | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'bitfinex',
      from: '2020-06-01',
      to: '2020-06-02',
      filters: [{ channel: 'book', symbols: ['BTCUSD'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Bitfinex real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/bybit">
<title>Bybit Derivatives | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'bybit',
      from: '2020-01-01',
      to: '2020-01-02',
      filters: [{ channel: 'orderBook_200', symbols: ['BTCUSD'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Bybit real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/okcoin">
<title>OKCoin | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'okcoin',
      from: '2020-01-01',
      to: '2020-01-02',
      filters: [{ channel: 'spot/trade', symbols: ['BTC-USD'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by OKCoin real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/coinflex">
<title>CoinFLEX (2.0) | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'coinflex',
      from: '2020-08-01',
      to: '2020-08-02',
      filters: [{ channel: 'futures/depth', symbols: ['BTC-USD-SWAP-LIN'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by CoinFLEX 2.0 real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/legal">
<title>Privacy Policy | Tardis.dev Documentation</title>
<content><paragraph>This Privacy Policy explains how Personal Information about our (potential) customers and other individuals using our services is collected, used and disclosed by Tardis.dev and its respective affiliates ("us", "we", "our" or "Tardis.dev"). This Privacy Policy describes our privacy practices in relation to the use of our websites (including any customer portal or interactive customer website) (https://tardis.devarrow-up-right and https://docs.tardis.devarrow-up-right), services, solutions, tools, and related applications, services, and programs, including research and marketing activities, offered by us (the "Services"), as well as your choices regarding use, access, storage and correction of Personal Information. It also describes how we collect, use, disclose and otherwise process Personal Information collected in relation to our Services and otherwise in the course of our business activities. This Privacy Policy does not apply to Personal Information collected about our employees, applicants or other personnel.</paragraph><paragraph>By using our Services or by agreeing to our Terms of Service required to use our Services, you agree to the collection, usage, storage and disclosure of information described in this Privacy Policy.</paragraph><paragraph>Our Services may contain links to other websites or services; and information practices and/or the content of such other websites or services shall be governed by the privacy statements of such other websites or services.</paragraph><paragraph>What information do we collect?</paragraph><paragraph>Here we describe what information we collect, what we use it for, which third parties we use to help us and give links to the privacy policies of those third parties for you to read:</paragraph><list type="ul"><item>we track and store usage behavior such as which links are clicked on and API endpoints usage statistics so we can optimize the services we provide to you. We use Google Analytics with anonymized IP feature enabled for this purpose. Please refer to their Privacy Statement https://www.google.com/policies/privacy/arrow-up-right</item><item>When you contact us for support or other customer service requests, we can maintain records related to such requests, including any information provided by you related to such support or service requests and contact you back about our services with relevant information. We may also obtain Personal Information about you from third parties, such as LinkedIn, Facebook, Twitter and other publicly accessible sources.</item><item>We may use your Personal Information to contact you with marketing or promotional materials and other information communications related to Tardis.dev. If you no longer wish to receive marketing or promotional communications related to us, you can at any moment in time by using the unsubscribe button in the email or emailing [email protected]envelope to request us to stop sending you such communications. Such a request will be processed immediately by us, but in any event within two (2) business days.</item><item>To keep track of which users should have access to paid versions of our services and to handle API authentication and authorization, we store user details such as email address, name, subscription details and IP address in Cloudflare data store (encrypted at rest). Please refer to their Privacy Statement https://www.cloudflare.com/privacypolicyarrow-up-right</item></list><paragraph>we track and store usage behavior such as which links are clicked on and API endpoints usage statistics so we can optimize the services we provide to you. We use Google Analytics with anonymized IP feature enabled for this purpose. Please refer to their Privacy Statement https://www.google.com/policies/privacy/arrow-up-right</paragraph><paragraph>When you contact us for support or other customer service requests, we can maintain records related to such requests, including any information provided by you related to such support or service requests and contact you back about our services with relevant information. We may also obtain Personal Information about you from third parties, such as LinkedIn, Facebook, Twitter and other publicly accessible sources.</paragraph><paragraph>We may use your Personal Information to contact you with marketing or promotional materials and other information communications related to Tardis.dev. If you no longer wish to receive marketing or promotional communications related to us, you can at any moment in time by using the unsubscribe button in the email or emailing [email protected]envelope to request us to stop sending you such communications. Such a request will be processed immediately by us, but in any event within two (2) business days.</paragraph><paragraph>To keep track of which users should have access to paid versions of our services and to handle API authentication and authorization, we store user details such as email address, name, subscription details and IP address in Cloudflare data store (encrypted at rest). Please refer to their Privacy Statement https://www.cloudflare.com/privacypolicyarrow-up-right</paragraph><paragraph>Unless specified otherwise, all data requested by Tardis.dev is mandatory and failure to provide this data may make it impossible for us to provide our services. In cases where Tardis.dev website specifically states that some data is not mandatory, users are free not to communicate this data without consequences to the availability or the functioning of the service. Users who are uncertain about which personal data is mandatory are welcome to contact us at [email protected]envelope.</paragraph><paragraph>We may publicly display aggregated anonymous data to help communicate what we know about how our services are typically used.</paragraph><paragraph>How do you get my consent?</paragraph><paragraph>When you provide us with personal information to make a purchase or return a purchase, we imply that you consent to us collecting this information and using it for that specific reason only.</paragraph><paragraph>If we ask for your personal information so that we can send you communications in the future, we will either ask you directly for your expressed consent or provide you with an opportunity to say no afterwards.</paragraph><paragraph>How do I withdraw my consent?</paragraph><paragraph>If you wish to withdraw the consent you have given to us to collect, store or use your information, please contact us at [email protected]envelope.</paragraph><paragraph>We may disclose your personal information if we are required by law to do so or if you violate our Terms of Service.</paragraph><paragraph>Third-party services</paragraph><paragraph>Third-party providers will collect, use and disclose your information to the extent necessary to allow them to perform the services they provide to us. Third-party service providers have their own privacy policies in respect to the information we are required to provide to them. For these providers, we recommend that you read their privacy policies so you can understand the manner in which your personal information will be handled by these providers.</paragraph><paragraph>In particular, remember that certain providers may be located in or have facilities that are located in a different jurisdiction than either you or us. Your information may become subject to the laws of the jurisdiction(s) in which a third party service provider or its facilities are located.</paragraph><paragraph>When you click on links that appear in any of the content we provide to you, those links may direct you to third party sites. We are not responsible for the privacy practices of other sites and encourage you to read their privacy statements.</paragraph><paragraph>To protect your personal information, we take reasonable precautions to make sure it is not inappropriately lost, misused, accessed, disclosed, altered or destroyed.</paragraph><paragraph>Our website uses cookiesarrow-up-right to track anonymized usage behavior and to personalize content.</paragraph><paragraph>Retention of Personal Information</paragraph><paragraph>We retain personal information that you provide us as long as we consider it potentially useful in contacting you about our services, or as needed to comply with our legal obligations, resolve disputes and enforce our agreements.</paragraph><paragraph>By using Tardis.dev website and services, you represent that you are at least the age of majority in your state or province of residence, or that you are the age of majority in your state or province of residence and you have given us your consent to allow any of your minor dependents to use this site.</paragraph><paragraph>Changes to this privacy policy</paragraph><paragraph>We reserve the right to modify this privacy policy at any time, so please review it frequently. Changes and clarifications will take effect immediately upon their posting on the website. If we make material changes to this policy, we will notify you here that it has been updated, so that you are aware of what information we collect, how we use it, and under what circumstances, if any, we use and/or disclose it.</paragraph><paragraph>If we are acquired or merged with another company, your information may be transferred to the new owners.</paragraph><paragraph>If you would like to access, correct, amend or delete any personal information we have about you, register a complaint, or simply want more information you should email us at [email protected]envelope.</paragraph></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/binance-delivery">
<title>Binance COIN Futures | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'binance-delivery',
      from: '2020-07-01',
      to: '2020-07-02',
      filters: [{ channel: 'depth', symbols: ['btcusd_200925'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Binance COIN Futures real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/okex-futures">
<title>OKX Futures | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'okex-futures',
      from: '2023-01-01',
      to: '2023-01-02',
      filters: [{ channel: 'books-l2-tbt', symbols: [] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by OKX real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/okex-swap">
<title>OKX Swap | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'okex-swap',
      from: '2020-01-01',
      to: '2020-01-02',
      filters: [{ channel: 'swap/trade', symbols: [] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by OKEx real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/huobi-dm-swap">
<title>Huobi COIN Swaps | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'huobi-dm-swap',
      from: '2020-04-01',
      to: '2020-04-02',
      filters: [{ channel: 'depth', symbols: ['BTC-USD'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Huobi COIN Swaps real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/huobi-dm-linear-swap">
<title>Huobi USDT Swaps | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'huobi-dm-linear-swap',
      from: '2020-04-01',
      to: '2020-04-02',
      filters: [{ channel: 'depth', symbols: ['BTC-USDT'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Huobi USDT Swaps real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/bitfinex-derivatives">
<title>Bitfinex Derivatives | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'bitfinex-derivatives',
      from: '2020-06-01',
      to: '2020-06-02',
      filters: [{ channel: 'book', symbols: ['BTCF0:USTF0'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Bitfinex real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/upbit">
<title>Upbit | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'upbit',
      from: '2021-09-01',
      to: '2021-09-02',
      filters: [{ channel: 'orderbook', symbols: ['KRW-BTC'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Upbit real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/dydx">
<title>dYdX | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'dydx',
      from: '2021-09-01',
      to: '2021-09-02',
      filters: [{ channel: 'v3_orderbook', symbols: ['BTC-USD'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by dYdX real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/phemex">
<title>Phemex | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'phemex',
      from: '2020-04-01',
      to: '2020-04-02',
      filters: [{ channel: 'book', symbols: ['BTCUSD'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Phemex real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/binance-us">
<title>Binance US | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'binance-us',
      from: '2019-12-01',
      to: '2019-12-02',
      filters: [{ channel: 'depth', symbols: ['bnbusd'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Binance US real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/ftx-us">
<title>FTX US | Tardis.dev Documentation</title>
<content><code language="python"># requires Python >=3.6
# pip install tardis-dev

from tardis_dev import datasets, get_exchange_details
import logging

# optionally enable debug logs
# logging.basicConfig(level=logging.DEBUG)

exchange = 'ftx-us'
exchange_details = get_exchange_details(exchange)   

# iterate over and download all data for every symbol
for symbol in exchange_details["datasets"]["symbols"]:
    # alternatively specify datatypes explicitly ['trades', 'incremental_book_L2', 'quotes'] etc
    # see available options https://docs.tardis.dev/downloadable-csv-files#data-types
    data_types = symbol["dataTypes"]  
    symbol_id = symbol["id"]
    from_date =  symbol["availableSince"]
    to_date = symbol["availableTo"]

    # skip groupped symbols
    if symbol_id in ['PERPETUALS', 'SPOT', 'FUTURES']:
        continue

    print(f"Downloading {exchange} {data_types} for {symbol_id} from {from_date} to {to_date}")

    # each CSV dataset format is documented at https://docs.tardis.dev/downloadable-csv-files#data-types
    # see https://docs.tardis.dev/downloadable-csv-files#download-via-client-libraries for full options docs
    datasets.download(
        exchange = exchange,
        data_types = data_types,
        from_date =  from_date,
        to_date = to_date,
        symbols = [symbol_id],
        # TODO set your API key here
        api_key = "YOUR_API_KEY",
        # path where CSV data will be downloaded into
        download_dir = "./datasets",
    )</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/gate-io-futures">
<title>Gate.io Futures | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'gate-io-futures',
      from: '2020-07-01',
      to: '2020-07-02',
      filters: [{ channel: 'order_book', symbols: ['BTC_USDT'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Gate.io Futures real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/huobi">
<title>Huobi Global | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'huobi',
      from: '2019-12-01',
      to: '2019-12-02',
      filters: [{ channel: 'depth', symbols: ['btcusdt'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Huobi real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/gate-io">
<title>Gate.io | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'gate-io',
      from: '2020-07-01',
      to: '2020-07-02',
      filters: [{ channel: 'depth', symbols: ['BTC_USDT'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Gate.io real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/bitget">
<title>Bitget Spot | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'bitget',
      from: '2024-12-01',
      to: '2024-12-02',
      filters: [{ channel: 'books15', symbols: ['BTCUSDT'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Bitget real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/hyperliquid">
<title>Hyperliquid | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'hyperliquid',
      from: '2024-12-01',
      to: '2024-12-02',
      filters: [{ channel: 'l2Book', symbols: ['BTC'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Hyperliquid real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/hitbtc">
<title>HitBTC (high caps) | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'hitbtc',
      from: '2020-01-01',
      to: '2020-01-02',
      filters: [{ channel: 'updateOrderbook', symbols: ['BTCUSD'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by HitBTC real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/bybit-spot">
<title>Bybit Spot | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'bybit',
      from: '2023-03-01',
      to: '2023-03-02',
      filters: [{ channel: 'trade', symbols: ['BTCUSDT'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Bybit Spot real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/binance-dex">
<title>Binance DEX | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'binance-dex',
      from: '2019-12-01',
      to: '2019-12-02',
      filters: [{ channel: 'marketDiff', symbols: [] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Binance DEX real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/woo-x">
<title>WOO X | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'woo-x',
      from: '2023-02-01',
      to: '2023-02-02',
      filters: [{ channel: 'trade', symbols: [] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by WOO X real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/kucoin">
<title>Kucoin Spot | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'kucoin',
      from: '2023-02-01',
      to: '2023-02-02',
      filters: [{ channel: 'market/match', symbols: [] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Kucoin Spot real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/blockchain-com">
<title>Blockchain.com | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'blockchain-com',
      from: '2023-03-01',
      to: '2023-03-02',
      filters: [{ channel: 'l2', symbols: [] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Blockchain.com exchange real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/ascendex">
<title>AscendEX (BitMax) | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'ascendex',
      from: '2023-03-01',
      to: '2023-03-02',
      filters: [{ channel: 'trades', symbols: ['BTC-PERP'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Ascendex real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/delta">
<title>Delta | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'delta',
      from: '2020-06-01',
      to: '2020-06-02',
      filters: [{ channel: 'l2_orderbook', symbols: ['BTCUSD'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Delta Exchange real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/bitnomial">
<title>Bitnomial | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'bitnomial',
      from: '2023-03-01',
      to: '2023-03-02',
      filters: [{ channel: 'trade', symbols: [] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Bitnomial exchange real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/bitget-futures">
<title>Bitget Futures | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'bitget-futures',
      from: '2024-12-01',
      to: '2024-12-02',
      filters: [{ channel: 'books15', symbols: ['BTCUSDT'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Bitget Futures real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/crypto-com">
<title>Crypto.com | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'crypto-com',
      from: '2023-03-01',
      to: '2023-03-02',
      filters: [{ channel: 'book', symbols: [] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Crypto.com exchange real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
<page url="https://docs.tardis.dev/historical-data-details/binance-jersey">
<title>Binance Jersey | Tardis.dev Documentation</title>
<content><code language="javascript">// npm install tardis-dev
const { replay } = require('tardis-dev');

async function run() {
  try {
    const messages = replay({
      exchange: 'binance-jersey',
      from: '2019-12-01',
      to: '2019-12-02',
      filters: [{ channel: 'depth', symbols: ['btcgbp'] }],
      apiKey: 'YOUR_API_KEY'
    });

    // messages as provided by Binance Jersey real-time stream
    for await (const { localTimestamp, message } of messages) {
      console.log(localTimestamp, message);
    }
  } catch (e) {
    console.error(e);
  }
}

run();</code></content>
</page>
</source>
</onefilellm_output>